{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain_chain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Master\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      4\u001b[0m master \u001b[38;5;241m=\u001b[39m Master(knowledge_base_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend/knowledge_store\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\vidit\\codes\\projects\\tutor-app\\backend\\src\\tutor\\components\\main_chain.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentLoader\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "File \u001b[1;32mc:\\vidit\\codes\\projects\\tutor-app\\backend\\src\\tutor\\components\\load_documents.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parser\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pinecone, ServerlessSpec\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex, SimpleDirectoryReader\n",
      "File \u001b[1;32mc:\\vidit\\codes\\projects\\tutor-app\\backend\\src\\tutor\\utils\\parsers.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_parse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaParse\n\u001b[0;32m      6\u001b[0m dotenv\u001b[38;5;241m.\u001b[39mload_dotenv()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mParser\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_parse\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_parse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaParse, ResultType\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaParse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_parse\\base.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Union\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BufferedIOBase\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_jobs\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbridge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, validator\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_BASE_URL\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\__init__.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmock_embed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MockEmbedding\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# indices\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# loading\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     ComposableGraph,\n\u001b[0;32m     21\u001b[0m     DocumentSummaryIndex,\n\u001b[0;32m     22\u001b[0m     GPTDocumentSummaryIndex,\n\u001b[0;32m     23\u001b[0m     GPTKeywordTableIndex,\n\u001b[0;32m     24\u001b[0m     GPTListIndex,\n\u001b[0;32m     25\u001b[0m     GPTRAKEKeywordTableIndex,\n\u001b[0;32m     26\u001b[0m     GPTSimpleKeywordTableIndex,\n\u001b[0;32m     27\u001b[0m     GPTTreeIndex,\n\u001b[0;32m     28\u001b[0m     GPTVectorStoreIndex,\n\u001b[0;32m     29\u001b[0m     KeywordTableIndex,\n\u001b[0;32m     30\u001b[0m     KnowledgeGraphIndex,\n\u001b[0;32m     31\u001b[0m     PropertyGraphIndex,\n\u001b[0;32m     32\u001b[0m     ListIndex,\n\u001b[0;32m     33\u001b[0m     RAKEKeywordTableIndex,\n\u001b[0;32m     34\u001b[0m     SimpleKeywordTableIndex,\n\u001b[0;32m     35\u001b[0m     SummaryIndex,\n\u001b[0;32m     36\u001b[0m     TreeIndex,\n\u001b[0;32m     37\u001b[0m     VectorStoreIndex,\n\u001b[0;32m     38\u001b[0m     load_graph_from_storage,\n\u001b[0;32m     39\u001b[0m     load_index_from_storage,\n\u001b[0;32m     40\u001b[0m     load_indices_from_storage,\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# structured\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstruct_store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     SQLDocumentContextBuilder,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\__init__.py:32\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPTListIndex, ListIndex, SummaryIndex\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlist\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     GPTListIndex,\n\u001b[0;32m     29\u001b[0m     ListIndex,\n\u001b[0;32m     30\u001b[0m     SummaryIndex,\n\u001b[0;32m     31\u001b[0m )\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     load_graph_from_storage,\n\u001b[0;32m     34\u001b[0m     load_index_from_storage,\n\u001b[0;32m     35\u001b[0m     load_indices_from_storage,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_modal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalVectorStoreIndex\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstruct_store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     GPTPandasIndex,\n\u001b[0;32m     40\u001b[0m     PandasIndex,\n\u001b[0;32m     41\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\loading.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseIndex\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomposability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComposableGraph\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m INDEX_STRUCT_TYPE_TO_INDEX_CLASS\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StorageContext\n\u001b[0;32m      9\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\registry.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknowledge_graph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KnowledgeGraphIndex\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlist\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryIndex\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_modal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalVectorStoreIndex\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperty_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PropertyGraphIndex\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstruct_store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PandasIndex\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\multi_modal\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Vector-store based data structures.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_modal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalVectorStoreIndex\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_modal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretriever\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      5\u001b[0m     MultiModalVectorIndexRetriever,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiModalVectorStoreIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiModalVectorIndexRetriever\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\multi_modal\\base.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMType\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_modal_llms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalLLM\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_modal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleMultiModalQueryEngine\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseNode, ImageNode\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceContext\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\query_engine\\__init__.py:41\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretry_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     RetryGuidelineQueryEngine,\n\u001b[0;32m     36\u001b[0m     RetryQueryEngine,\n\u001b[0;32m     37\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretry_source_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     RetrySourceQueryEngine,\n\u001b[0;32m     40\u001b[0m )\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrouter_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     42\u001b[0m     RetrieverRouterQueryEngine,\n\u001b[0;32m     43\u001b[0m     RouterQueryEngine,\n\u001b[0;32m     44\u001b[0m     ToolRetrieverRouterQueryEngine,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql_join_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLJoinQueryEngine\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql_vector_query_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     SQLAutoVectorQueryEngine,\n\u001b[0;32m     49\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\query_engine\\router_query_engine.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse_synthesizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TreeSummarize\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseNode, QueryBundle\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_selector_from_llm\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceContext\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     Settings,\n\u001b[0;32m     29\u001b[0m     callback_manager_from_settings_or_context,\n\u001b[0;32m     30\u001b[0m     llm_from_settings_or_context,\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\selectors\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_selectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingSingleSelector\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_selectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     LLMMultiSelector,\n\u001b[0;32m      4\u001b[0m     LLMSingleSelector,\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselectors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_selectors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     PydanticMultiSelector,\n\u001b[0;32m      8\u001b[0m     PydanticSingleSelector,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMSingleSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMMultiSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPydanticMultiSelector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m ]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tutor.components.main_chain import Master\n",
    "import json\n",
    "\n",
    "master = Master(knowledge_base_path=\"backend/knowledge_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upserting data to pinecone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 502/502 [00:11<00:00, 41.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upserted data to pinecone\n",
      "Successfully stored to database\n",
      "Tool made\n",
      "Master has learnt the study materials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Learning Successful', True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.learn(index_name='tutor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"backend/src/tutor/templates/quiz_template.json\", 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "\n",
    "response_template = json.dumps(json_data, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"1\": {\\n        \"no\": \"1\",\\n        \"mcq\": \"multiple choice questions\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    },\\n    \"2\": {\\n        \"no\": \"2\",\\n        \"mcq\": \"multiple choice questions\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    },\\n    \"3\": {\\n        \"no\": \"3\",\\n        \"mcq\": \"multiple choice questions\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    }\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools :  [functools.partial(StructuredTool(name='RetrieveDataTool', description='Tool for retrieving data based on topics from a provided retrieval index.\\nInput should be the topics separated by comma.\\nUseful for getting context from the index.\\n\\nArgs:\\n    retrieval_index: The VectorStoreIndex to query from.\\n    topics (str): Topic names separated by comma.\\n\\nReturns:\\n    str: Retrieved data joined by newlines.', args_schema=<class 'pydantic.v1.main.RetrieveDataToolSchema'>, return_direct=True, func=<function retrieve_data at 0x000002B2A63A6EF0>), <llama_index.core.indices.vector_store.base.VectorStoreIndex object at 0x000002B2A827BD00>)]\n",
      "> Running step bd062bf8-da0f-4278-b2eb-5ec88e1eb28a. Step input: \n",
      "You are an expert mcq maker. Fetch and use the study materials given by the user, it is your job to create mcq of 10 questions for topics Transformers, vision transformers of easy difficulty. \n",
      "Strictly fetch and use only the study materials provided by the user. Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 10 MCQs.\n",
      "Make sure correct answers are not always the same option for each question.\n",
      "### RESPONSE_JSON\n",
      "{\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'functools.partial' object has no attribute 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m json_str \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(json_file)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# prompt = \"Give me the information about topic transformers and vision transformers\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmaster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_mcq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\vidit\\codes\\projects\\tutor-app\\backend\\src\\tutor\\components\\main_chain.py:100\u001b[0m, in \u001b[0;36mMaster.generate_mcq\u001b[1;34m(self, json_str)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# print(\"Formatted prompt :\",formatted_prompt)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m mcq_agent \u001b[38;5;241m=\u001b[39m ReActAgent\u001b[38;5;241m.\u001b[39mfrom_tools(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools, llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgemini_llm, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, context\u001b[38;5;241m=\u001b[39mmcq_agent_context)\n\u001b[1;32m--> 100\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmcq_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatted_prompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\base\\base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[1;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[1;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[0;32m     55\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\base\\agent\\types.py:44\u001b[0m, in \u001b[0;36mBaseAgent._query\u001b[1;34m(self, query_bundle)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, query_bundle: QueryBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RESPONSE_TYPE:\n\u001b[1;32m---> 44\u001b[0m     agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m     49\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(agent_response), source_nodes\u001b[38;5;241m=\u001b[39magent_response\u001b[38;5;241m.\u001b[39msource_nodes\n\u001b[0;32m     50\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:640\u001b[0m, in \u001b[0;36mAgentRunner.chat\u001b[1;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[0;32m    635\u001b[0m     tool_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_tool_choice\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    637\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mAGENT_STEP,\n\u001b[0;32m    638\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mMESSAGES: [message]},\n\u001b[0;32m    639\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 640\u001b[0m     chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chat_response, AgentChatResponse)\n\u001b[0;32m    647\u001b[0m     e\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: chat_response})\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:572\u001b[0m, in \u001b[0;36mAgentRunner._chat\u001b[1;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[0;32m    569\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(AgentChatWithStepStartEvent(user_msg\u001b[38;5;241m=\u001b[39mmessage))\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_step_output\u001b[38;5;241m.\u001b[39mis_last:\n\u001b[0;32m    577\u001b[0m         result_output \u001b[38;5;241m=\u001b[39m cur_step_output\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:411\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[1;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# not clear when you would do that by theoretically possible\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mWAIT:\n\u001b[1;32m--> 411\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mrun_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m ChatResponseMode\u001b[38;5;241m.\u001b[39mSTREAM:\n\u001b[0;32m    413\u001b[0m     cur_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_worker\u001b[38;5;241m.\u001b[39mstream_step(step, task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[1;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_enter(\n\u001b[0;32m    227\u001b[0m     id_\u001b[38;5;241m=\u001b[39mid_, bound_args\u001b[38;5;241m=\u001b[39mbound_args, instance\u001b[38;5;241m=\u001b[39minstance, parent_id\u001b[38;5;241m=\u001b[39mparent_id\n\u001b[0;32m    228\u001b[0m )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent(SpanDropEvent(span_id\u001b[38;5;241m=\u001b[39mid_, err_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m cast(CallbackManager, callback_manager)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mas_trace(trace_id):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:763\u001b[0m, in \u001b[0;36mReActAgentWorker.run_step\u001b[1;34m(self, step, task, **kwargs)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, step: TaskStep, task: Task, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskStepOutput:\n\u001b[0;32m    762\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:541\u001b[0m, in \u001b[0;36mReActAgentWorker._run_step\u001b[1;34m(self, step, task)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# TODO: see if we want to do step-based inputs\u001b[39;00m\n\u001b[0;32m    540\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tools(task\u001b[38;5;241m.\u001b[39minput)\n\u001b[1;32m--> 541\u001b[0m input_chat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_react_chat_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnew_memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_reasoning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent_reasoning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# send prompt\u001b[39;00m\n\u001b[0;32m    549\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mchat(input_chat)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\agent\\react\\formatter.py:68\u001b[0m, in \u001b[0;36mReActChatFormatter.format\u001b[1;34m(self, tools, chat_history, current_reasoning)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format chat history into list of ChatMessage.\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m current_reasoning \u001b[38;5;241m=\u001b[39m current_reasoning \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m     67\u001b[0m format_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_desc\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mget_react_tool_descriptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_names\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tool\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget_name() \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]),\n\u001b[0;32m     70\u001b[0m }\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext:\n\u001b[0;32m     72\u001b[0m     format_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\agent\\react\\formatter.py:27\u001b[0m, in \u001b[0;36mget_react_tool_descriptions\u001b[1;34m(tools)\u001b[0m\n\u001b[0;32m     24\u001b[0m tool_descs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools:\n\u001b[0;32m     26\u001b[0m     tool_desc \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> Tool Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool Args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mfn_schema_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m     )\n\u001b[0;32m     31\u001b[0m     tool_descs\u001b[38;5;241m.\u001b[39mappend(tool_desc)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tool_descs\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\tools\\types.py:192\u001b[0m, in \u001b[0;36mBaseToolAsyncAdapter.metadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ToolMetadata:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'functools.partial' object has no attribute 'metadata'"
     ]
    }
   ],
   "source": [
    "json_file = {\n",
    "            \"number\": 10,\n",
    "            \"topics\": \"Transformers, vision transformers\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"response_template\": response_template,\n",
    "            \"question_type\": \"mcq\"\n",
    "        }\n",
    "json_str = json.dumps(json_file)\n",
    "\n",
    "# prompt = \"Give me the information about topic transformers and vision transformers\"\n",
    "master.generate_mcq(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tutor.components.main_chain import Master\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import uuid\n",
    "from uuid import UUID, uuid4\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class format(BaseModel):\n",
    "    id: Optional[UUID] = None\n",
    "    number: Optional[int] = 10\n",
    "    topics: str\n",
    "    difficulty: Optional[str] = \"easy\"\n",
    "    response_template: Optional[str] = \"\"\n",
    "    question_type: str = \"mcq\"\n",
    "\n",
    "\n",
    "\n",
    "def store(FOLDER_PATH):\n",
    "    try:\n",
    "        master = Master(knowledge_base_path=FOLDER_PATH)\n",
    "        master.learn(index_name='tutor')\n",
    "        \n",
    "        return {True,\"Stored the file\",master}\n",
    "    except:\n",
    "        return {False,\"Error in storing the file\",None}\n",
    "\n",
    "def run(master,task):\n",
    "\n",
    "    return master.generate_mcq(task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing the data\n",
      "Data parsed\n",
      "Storing the data\n",
      "upserting data to pinecone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 597/597 [00:14<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upserted data to pinecone\n",
      "Successfully stored to database\n",
      "Data stored\n",
      "Tool made\n",
      "Master has learnt the study materials.\n",
      "{\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    }\n",
      "}\n",
      "Tools :  [<llama_index.core.tools.function_tool.FunctionTool object at 0x0000014EC7E26800>]\n",
      "Formatted prompt : \n",
      "You are an expert mcq maker. Fetch and use the study materials given by the user, it is your job to create mcq of 10 questions for topics transformers, vision transformers of easy difficulty. \n",
      "Strictly fetch and use only the study materials provided by the user. Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 10 MCQs.\n",
      "Make sure correct answers are not always the same option for each question.\n",
      "### RESPONSE_JSON\n",
      "{\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "> Running step befd707e-a6ab-4f44-8a84-596e974da838. Step input: \n",
      "You are an expert mcq maker. Fetch and use the study materials given by the user, it is your job to create mcq of 10 questions for topics transformers, vision transformers of easy difficulty. \n",
      "Strictly fetch and use only the study materials provided by the user. Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 10 MCQs.\n",
      "Make sure correct answers are not always the same option for each question.\n",
      "### RESPONSE_JSON\n",
      "{\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: english. I need to use a tool to help me answer the question.\n",
      "Action: RetrieveDataTool\n",
      "Action Input: {'prompt': 'transformers, vision transformers'}\n",
      "\u001b[0mnumber_of_chunks:  40\n",
      "chunk:  0\n",
      "Chunk Length:  284\n",
      "Transformers, ResNets, and hybrids.\n",
      "\n",
      "Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\n",
      "\n",
      "|Linear 5-shot ImageNet Top1 [%]Average-5|ImageNet| | | |\n",
      "chunk:  1\n",
      "Chunk Length:  284\n",
      "Transformers, ResNets, and hybrids.\n",
      "\n",
      "Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\n",
      "\n",
      "|Linear 5-shot ImageNet Top1 [%]Average-5|ImageNet| | | |\n",
      "chunk:  2\n",
      "Chunk Length:  342\n",
      "At the cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention and although the sequence length that self-attention operates on is smaller in axial case, there is an extra MLP per Axial-ViT block.\n",
      "chunk:  3\n",
      "Chunk Length:  271\n",
      "# 4.5 INSPECTING VISION TRANSFORMER\n",
      "\n",
      "To begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1).\n",
      "chunk:  4\n",
      "Chunk Length:  380\n",
      "# Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "chunk:  5\n",
      "Chunk Length:  271\n",
      "# 4.5 INSPECTING VISION TRANSFORMER\n",
      "\n",
      "To begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1).\n",
      "chunk:  6\n",
      "Chunk Length:  380\n",
      "# Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "chunk:  7\n",
      "Chunk Length:  342\n",
      "At the cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention and although the sequence length that self-attention operates on is smaller in axial case, there is an extra MLP per Axial-ViT block.\n",
      "chunk:  8\n",
      "Chunk Length:  214\n",
      "The tasks are divided into three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite imagery, and Structured – tasks that require geometric understanding like localization.\n",
      "chunk:  9\n",
      "Chunk Length:  247\n",
      "D.2 TRANSFORMER SHAPE\n",
      "\n",
      "We ran ablations on scaling different dimensions of the Transformer architecture to find out which\n",
      "are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\n",
      "for different configurations.\n",
      "chunk:  10\n",
      "Chunk Length:  383\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "chunk:  11\n",
      "Chunk Length:  250\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "chunk:  12\n",
      "Chunk Length:  247\n",
      "D.2 TRANSFORMER SHAPE\n",
      "\n",
      "We ran ablations on scaling different dimensions of the Transformer architecture to find out which\n",
      "are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\n",
      "for different configurations.\n",
      "chunk:  13\n",
      "Chunk Length:  162\n",
      "In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-attention in order to be applicable to images.\n",
      "chunk:  14\n",
      "Chunk Length:  383\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "chunk:  15\n",
      "Chunk Length:  352\n",
      "Second, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n",
      "chunk:  16\n",
      "Chunk Length:  250\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "chunk:  17\n",
      "Chunk Length:  269\n",
      "The output of this token is then trans-\n",
      "formed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\n",
      "in the single hidden layer.\n",
      "This design is inherited from the Transformer model for text, and we use it throughout the main\n",
      "paper.\n",
      "chunk:  18\n",
      "Chunk Length:  352\n",
      "Second, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n",
      "chunk:  19\n",
      "Chunk Length:  271\n",
      "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "# Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "chunk:  20\n",
      "Chunk Length:  298\n",
      "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to\n",
      "images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of\n",
      "linear embeddings of these patches as an input to a Transformer.\n",
      "chunk:  21\n",
      "Chunk Length:  196\n",
      "5) is typically set to D/k.\n",
      "\n",
      "MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa Umsa ∈ Rk·Dh×D\n",
      "\n",
      "# B EXPERIMENT DETAILS\n",
      "\n",
      "# B.1 TRAINING\n",
      "\n",
      "Table 3 summarizes our training setups for our different models.\n",
      "chunk:  22\n",
      "Chunk Length:  161\n",
      "# RELATED WORK\n",
      "\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.\n",
      "chunk:  23\n",
      "Chunk Length:  214\n",
      "The tasks are divided into three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite imagery, and Structured – tasks that require geometric understanding like localization.\n",
      "chunk:  24\n",
      "Chunk Length:  269\n",
      "The output of this token is then trans-\n",
      "formed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\n",
      "in the single hidden layer.\n",
      "This design is inherited from the Transformer model for text, and we use it throughout the main\n",
      "paper.\n",
      "chunk:  25\n",
      "Chunk Length:  161\n",
      "# RELATED WORK\n",
      "\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.\n",
      "chunk:  26\n",
      "Chunk Length:  199\n",
      "Globally, we find that the model attends to image regions that are semantically relevant for classification (Figure 6).\n",
      "\n",
      "# 4.6 SELF-SUPERVISION\n",
      "\n",
      "Transformers show impressive performance on NLP tasks.\n",
      "chunk:  27\n",
      "Chunk Length:  228\n",
      "Dirk Weissenborn, and Tim Salimans|Axial attention in multidimensional transformers|arXiv|2019|\n",
      "|Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei|Relation networks for object detection|CVPR|2018|\n",
      "|Han Hu, Zheng Zhang,\n",
      "chunk:  28\n",
      "Chunk Length:  230\n",
      "Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang\n",
      "et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al.,\n",
      "2020a).\n",
      "chunk:  29\n",
      "Chunk Length:  318\n",
      "Cosine similarity\n",
      "\n",
      "CONCLUSION\n",
      "\n",
      "Mean attention distance (pixels)\n",
      "We have explored pe direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into pe architecture apart from pe initial patch extraction step.\n",
      "chunk:  30\n",
      "Chunk Length:  198\n",
      "pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs;\n",
      "chunk:  31\n",
      "Chunk Length:  272\n",
      "On the Specialized the performance of the top two models is similar.\n",
      "\n",
      "# PRE-TRAINING DATA REQUIREMENTS\n",
      "\n",
      "The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer inductive biases for vision than ResNets, how crucial is the dataset size?\n",
      "chunk:  32\n",
      "Chunk Length:  269\n",
      "Multihead self-attention (MSA) is an extension of SA in which we run k self-attention operations, called “heads”, in parallel, and project their concatenated outputs. To keep compute and number of parameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\n",
      "chunk:  33\n",
      "Chunk Length:  325\n",
      "We show that\n",
      "this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform\n",
      "very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized\n",
      "or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.\n",
      "chunk:  34\n",
      "Chunk Length:  205\n",
      "Daniel Keysers, Mario Luˇc, Noamci´ Shazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In ACL, 2020.\n",
      "chunk:  35\n",
      "Chunk Length:  162\n",
      "In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-attention in order to be applicable to images.\n",
      "chunk:  36\n",
      "Chunk Length:  181\n",
      "For fine-tuning we use SGD with momentum, batch size 512, for all models. For ImageNet results in Table 2, we fine-tuned at higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14.\n",
      "chunk:  37\n",
      "Chunk Length:  196\n",
      "5) is typically set to D/k.\n",
      "\n",
      "MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa Umsa ∈ Rk·Dh×D\n",
      "\n",
      "# B EXPERIMENT DETAILS\n",
      "\n",
      "# B.1 TRAINING\n",
      "\n",
      "Table 3 summarizes our training setups for our different models.\n",
      "chunk:  38\n",
      "Chunk Length:  230\n",
      "Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang\n",
      "et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al.,\n",
      "2020a).\n",
      "chunk:  39\n",
      "Chunk Length:  271\n",
      "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "# Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "\u001b[1;3;34mObservation: Transformers, ResNets, and hybrids.\n",
      "\n",
      "Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\n",
      "\n",
      "|Linear 5-shot ImageNet Top1 [%]Average-5|ImageNet| | | |\n",
      "\n",
      "Transformers, ResNets, and hybrids.\n",
      "\n",
      "Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\n",
      "\n",
      "|Linear 5-shot ImageNet Top1 [%]Average-5|ImageNet| | | |\n",
      "\n",
      "At the cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention and although the sequence length that self-attention operates on is smaller in axial case, there is an extra MLP per Axial-ViT block.\n",
      "\n",
      "# 4.5 INSPECTING VISION TRANSFORMER\n",
      "\n",
      "To begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1).\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "\n",
      "# 4.5 INSPECTING VISION TRANSFORMER\n",
      "\n",
      "To begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1).\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "\n",
      "At the cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention and although the sequence length that self-attention operates on is smaller in axial case, there is an extra MLP per Axial-ViT block.\n",
      "\n",
      "The tasks are divided into three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite imagery, and Structured – tasks that require geometric understanding like localization.\n",
      "\n",
      "D.2 TRANSFORMER SHAPE\n",
      "\n",
      "We ran ablations on scaling different dimensions of the Transformer architecture to find out which\n",
      "are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\n",
      "for different configurations.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "D.2 TRANSFORMER SHAPE\n",
      "\n",
      "We ran ablations on scaling different dimensions of the Transformer architecture to find out which\n",
      "are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\n",
      "for different configurations.\n",
      "\n",
      "In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-attention in order to be applicable to images.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "Second, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n",
      "\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "The output of this token is then trans-\n",
      "formed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\n",
      "in the single hidden layer.\n",
      "This design is inherited from the Transformer model for text, and we use it throughout the main\n",
      "paper.\n",
      "\n",
      "Second, hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models. This result is somewhat surprising, since one might expect convolutional local feature processing to assist ViT at any size. Third, Vision Transformers appear not to saturate within the range tried, motivating future scaling efforts.\n",
      "\n",
      "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "# Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "\n",
      "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to\n",
      "images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of\n",
      "linear embeddings of these patches as an input to a Transformer.\n",
      "\n",
      "5) is typically set to D/k.\n",
      "\n",
      "MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa Umsa ∈ Rk·Dh×D\n",
      "\n",
      "# B EXPERIMENT DETAILS\n",
      "\n",
      "# B.1 TRAINING\n",
      "\n",
      "Table 3 summarizes our training setups for our different models.\n",
      "\n",
      "# RELATED WORK\n",
      "\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.\n",
      "\n",
      "The tasks are divided into three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite imagery, and Structured – tasks that require geometric understanding like localization.\n",
      "\n",
      "The output of this token is then trans-\n",
      "formed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\n",
      "in the single hidden layer.\n",
      "This design is inherited from the Transformer model for text, and we use it throughout the main\n",
      "paper.\n",
      "\n",
      "# RELATED WORK\n",
      "\n",
      "Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.\n",
      "\n",
      "Globally, we find that the model attends to image regions that are semantically relevant for classification (Figure 6).\n",
      "\n",
      "# 4.6 SELF-SUPERVISION\n",
      "\n",
      "Transformers show impressive performance on NLP tasks.\n",
      "\n",
      "Dirk Weissenborn, and Tim Salimans|Axial attention in multidimensional transformers|arXiv|2019|\n",
      "|Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei|Relation networks for object detection|CVPR|2018|\n",
      "|Han Hu, Zheng Zhang,\n",
      "\n",
      "Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang\n",
      "et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al.,\n",
      "2020a).\n",
      "\n",
      "Cosine similarity\n",
      "\n",
      "CONCLUSION\n",
      "\n",
      "Mean attention distance (pixels)\n",
      "We have explored pe direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into pe architecture apart from pe initial patch extraction step.\n",
      "\n",
      "pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained for 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus L/16 and H/14 pre-trained for 14 epochs;\n",
      "\n",
      "On the Specialized the performance of the top two models is similar.\n",
      "\n",
      "# PRE-TRAINING DATA REQUIREMENTS\n",
      "\n",
      "The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer inductive biases for vision than ResNets, how crucial is the dataset size?\n",
      "\n",
      "Multihead self-attention (MSA) is an extension of SA in which we run k self-attention operations, called “heads”, in parallel, and project their concatenated outputs. To keep compute and number of parameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\n",
      "\n",
      "We show that\n",
      "this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform\n",
      "very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized\n",
      "or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.\n",
      "\n",
      "Daniel Keysers, Mario Luˇc, Noamci´ Shazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In ACL, 2020.\n",
      "\n",
      "In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-attention in order to be applicable to images.\n",
      "\n",
      "For fine-tuning we use SGD with momentum, batch size 512, for all models. For ImageNet results in Table 2, we fine-tuned at higher resolution: 512 for ViT-L/16 and 518 for ViT-H/14.\n",
      "\n",
      "5) is typically set to D/k.\n",
      "\n",
      "MSA(z) = [SA1(z); SA2(z); · · · ; SAk(z)] Umsa Umsa ∈ Rk·Dh×D\n",
      "\n",
      "# B EXPERIMENT DETAILS\n",
      "\n",
      "# B.1 TRAINING\n",
      "\n",
      "Table 3 summarizes our training setups for our different models.\n",
      "\n",
      "Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang\n",
      "et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al.,\n",
      "2020a).\n",
      "\n",
      "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "# Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "\u001b[0m> Running step 1da4a295-af66-458a-9d46-44bf0de0ef4c. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: {\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"Which of the following is NOT an advantage of using Transformers in image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Faster training time\",\n",
      "            \"b\": \"Improved accuracy\",\n",
      "            \"c\": \"Reduced computational cost\",\n",
      "            \"d\": \"Ability to process images of varying sizes\"\n",
      "        },\n",
      "        \"correct\": \"c\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"What is the main difference between a Transformer and a Convolutional Neural Network (CNN)?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Transformers use self-attention while CNNs use convolutions\",\n",
      "            \"b\": \"Transformers are fully connected while CNNs are not\",\n",
      "            \"c\": \"Transformers can only process images while CNNs can process both images and text\",\n",
      "            \"d\": \"Transformers are more computationally expensive than CNNs\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"Which of the following is a key component of a Transformer?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Encoder\",\n",
      "            \"b\": \"Decoder\",\n",
      "            \"c\": \"Self-attention mechanism\",\n",
      "            \"d\": \"Convolutional layer\"\n",
      "        },\n",
      "        \"correct\": \"c\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"no\": \"4\",\n",
      "        \"mcq\": \"What is the purpose of the positional encoding in a Transformer?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"To provide information about the order of the elements in the input sequence\",\n",
      "            \"b\": \"To improve the accuracy of the model\",\n",
      "            \"c\": \"To reduce the computational cost of the model\",\n",
      "            \"d\": \"To prevent overfitting\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"no\": \"5\",\n",
      "        \"mcq\": \"Which of the following is a potential limitation of using Transformers in image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They can be computationally expensive\",\n",
      "            \"b\": \"They are not as accurate as CNNs\",\n",
      "            \"c\": \"They can only process images of a fixed size\",\n",
      "            \"d\": \"They are difficult to train\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"no\": \"6\",\n",
      "        \"mcq\": \"What is the main advantage of using Vision Transformers over traditional CNNs?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They can process images of varying sizes\",\n",
      "            \"b\": \"They are more accurate\",\n",
      "            \"c\": \"They are faster to train\",\n",
      "            \"d\": \"They require less computational resources\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"no\": \"7\",\n",
      "        \"mcq\": \"Which of the following is a key difference between Axial Transformers and traditional Transformers?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Axial Transformers use row and column self-attention instead of global self-attention\",\n",
      "            \"b\": \"Axial Transformers are more computationally efficient\",\n",
      "            \"c\": \"Axial Transformers are more accurate\",\n",
      "            \"d\": \"Axial Transformers can only process images of a fixed size\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"no\": \"8\",\n",
      "        \"mcq\": \"What is the purpose of the MLP layer in a Transformer?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"To project the output of the self-attention mechanism to a higher dimension\",\n",
      "            \"b\": \"To reduce the dimensionality of the input sequence\",\n",
      "            \"c\": \"To add non-linearity to the model\",\n",
      "            \"d\": \"To perform classification\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"no\": \"9\",\n",
      "        \"mcq\": \"Which of the following is a potential application of Transformers in image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Object detection\",\n",
      "            \"b\": \"Image segmentation\",\n",
      "            \"c\": \"Image classification\",\n",
      "            \"d\": \"All of the above\"\n",
      "        },\n",
      "        \"correct\": \"d\"\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"no\": \"10\",\n",
      "        \"mcq\": \"What is the main challenge in applying Transformers to image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"The high computational cost\",\n",
      "            \"b\": \"The difficulty in capturing spatial relationships\",\n",
      "            \"c\": \"The lack of inductive biases for vision\",\n",
      "            \"d\": \"All of the above\"\n",
      "        },\n",
      "        \"correct\": \"d\"\n",
      "    }\n",
      "}\n",
      "\u001b[0m{\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"Which of the following is NOT an advantage of using Transformers in image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Faster training time\",\n",
      "            \"b\": \"Improved accuracy\",\n",
      "            \"c\": \"Reduced computational cost\",\n",
      "            \"d\": \"Ability to process images of varying sizes\"\n",
      "        },\n",
      "        \"correct\": \"c\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"What is the main difference between a Transformer and a Convolutional Neural Network (CNN)?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Transformers use self-attention while CNNs use convolutions\",\n",
      "            \"b\": \"Transformers are fully connected while CNNs are not\",\n",
      "            \"c\": \"Transformers can only process images while CNNs can process both images and text\",\n",
      "            \"d\": \"Transformers are more computationally expensive than CNNs\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"Which of the following is a key component of a Transformer?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Encoder\",\n",
      "            \"b\": \"Decoder\",\n",
      "            \"c\": \"Self-attention mechanism\",\n",
      "            \"d\": \"Convolutional layer\"\n",
      "        },\n",
      "        \"correct\": \"c\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"no\": \"4\",\n",
      "        \"mcq\": \"What is the purpose of the positional encoding in a Transformer?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"To provide information about the order of the elements in the input sequence\",\n",
      "            \"b\": \"To improve the accuracy of the model\",\n",
      "            \"c\": \"To reduce the computational cost of the model\",\n",
      "            \"d\": \"To prevent overfitting\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"no\": \"5\",\n",
      "        \"mcq\": \"Which of the following is a potential limitation of using Transformers in image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They can be computationally expensive\",\n",
      "            \"b\": \"They are not as accurate as CNNs\",\n",
      "            \"c\": \"They can only process images of a fixed size\",\n",
      "            \"d\": \"They are difficult to train\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"no\": \"6\",\n",
      "        \"mcq\": \"What is the main advantage of using Vision Transformers over traditional CNNs?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They can process images of varying sizes\",\n",
      "            \"b\": \"They are more accurate\",\n",
      "            \"c\": \"They are faster to train\",\n",
      "            \"d\": \"They require less computational resources\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"no\": \"7\",\n",
      "        \"mcq\": \"Which of the following is a key difference between Axial Transformers and traditional Transformers?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Axial Transformers use row and column self-attention instead of global self-attention\",\n",
      "            \"b\": \"Axial Transformers are more computationally efficient\",\n",
      "            \"c\": \"Axial Transformers are more accurate\",\n",
      "            \"d\": \"Axial Transformers can only process images of a fixed size\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"no\": \"8\",\n",
      "        \"mcq\": \"What is the purpose of the MLP layer in a Transformer?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"To project the output of the self-attention mechanism to a higher dimension\",\n",
      "            \"b\": \"To reduce the dimensionality of the input sequence\",\n",
      "            \"c\": \"To add non-linearity to the model\",\n",
      "            \"d\": \"To perform classification\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"no\": \"9\",\n",
      "        \"mcq\": \"Which of the following is a potential application of Transformers in image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Object detection\",\n",
      "            \"b\": \"Image segmentation\",\n",
      "            \"c\": \"Image classification\",\n",
      "            \"d\": \"All of the above\"\n",
      "        },\n",
      "        \"correct\": \"d\"\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"no\": \"10\",\n",
      "        \"mcq\": \"What is the main challenge in applying Transformers to image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"The high computational cost\",\n",
      "            \"b\": \"The difficulty in capturing spatial relationships\",\n",
      "            \"c\": \"The lack of inductive biases for vision\",\n",
      "            \"d\": \"All of the above\"\n",
      "        },\n",
      "        \"correct\": \"d\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    _, _, master = store(FOLDER_PATH=\"backend/artifacts\")\n",
    "    \n",
    "    \n",
    "    with open(\"backend/src/tutor/templates/quiz_template.json\", 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "\n",
    "    response_template = json.dumps(json_data, indent=4) \n",
    "        \n",
    "    print(response_template)\n",
    "\n",
    "\n",
    "    json_file = {\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"number\": 10,\n",
    "        \"topics\": \"transformers, vision transformers\",\n",
    "        \"difficulty\": \"easy\",\n",
    "        \"response_template\": response_template,\n",
    "        \"question_type\": \"mcq\"\n",
    "    }\n",
    "\n",
    "    # json_str = json.dumps(json_file)\n",
    "    task = format(**json_file)\n",
    "    \n",
    "    result = run(master,task)\n",
    "    \n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
