{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tutor.components.main_chain import Master\n",
    "import json\n",
    "\n",
    "master = Master(knowledge_base_path=\"backend/knowledge_store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upserting data to pinecone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 510/510 [00:12<00:00, 41.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upserted data to pinecone\n",
      "Successfully stored to database\n",
      "Tool made\n",
      "Master has learnt the study materials.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.learn(index_name='tutor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"backend/src/tutor/templates/quiz_template.json\", 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "\n",
    "response_template = json.dumps(json_data, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"1\": {\\n        \"no\": \"1\",\\n        \"mcq\": \"multiple choice questions\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    },\\n    \"2\": {\\n        \"no\": \"2\",\\n        \"mcq\": \"multiple choice questions\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    },\\n    \"3\": {\\n        \"no\": \"3\",\\n        \"mcq\": \"multiple choice questions\",\\n        \"options\": {\\n            \"a\": \"choice here\",\\n            \"b\": \"choice here\",\\n            \"c\": \"choice here\",\\n            \"d\": \"choice here\"\\n        },\\n        \"correct\": \"correct answer\"\\n    }\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools :  [<llama_index.core.tools.function_tool.FunctionTool object at 0x000001AEE0E24E50>]\n",
      "> Running step 7743bc03-ab32-489d-916f-5c2e61864cd4. Step input: \n",
      "You are an expert MCQ maker. Fetch and use the study materials given by the user, it is your job to create a quiz  of 10 multiple choice questions for topics Transformers, vision transformers in easy difficulty. \n",
      "Strictly fetch and use only the study materials provided by the user. Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 10 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"multiple choice questions\",\n",
      "        \"options\": {\n",
      "            \"a\": \"choice here\",\n",
      "            \"b\": \"choice here\",\n",
      "            \"c\": \"choice here\",\n",
      "            \"d\": \"choice here\"\n",
      "        },\n",
      "        \"correct\": \"correct answer\"\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: english. I need to use a tool to help me answer the question.\n",
      "Action: RetrieveDataTool\n",
      "Action Input: {'topics': 'Transformers, vision transformers'}\n",
      "\u001b[0mnumber_of_chunks:  20\n",
      "225\n",
      "349\n",
      "248\n",
      "380\n",
      "342\n",
      "383\n",
      "271\n",
      "231\n",
      "249\n",
      "162\n",
      "298\n",
      "121\n",
      "318\n",
      "250\n",
      "230\n",
      "224\n",
      "199\n",
      "325\n",
      "271\n",
      "219\n",
      "\u001b[1;3;34mObservation: Transformers, ResNets, and hybrids.\n",
      "\n",
      "Vision Transformers generally outperform ResNets with the same computational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap vanishes for larger models.\n",
      "\n",
      "), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP).\n",
      "\n",
      "Specialized, and Structured task groups.\n",
      "\n",
      "The Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer inductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of experiments.\n",
      "\n",
      "# Conclusion\n",
      "\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
      "\n",
      "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.\n",
      "\n",
      "At the cost of more compute. This is because in Axial-ViT models, each Transformer block with global self-attention is replaced by two Axial Transformer blocks, one with row and one with column self-attention and although the sequence length that self-attention operates on is smaller in axial case, there is an extra MLP per Axial-ViT block.\n",
      "\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "# 4.5 INSPECTING VISION TRANSFORMER\n",
      "\n",
      "To begin to understand how the Vision Transformer processes image data, we analyze its internal representations. The first layer of the Vision Transformer linearly projects the flattened patches into a lower-dimensional space (Eq. 1).\n",
      "\n",
      "The tasks are divided into three groups: Natural – tasks like the above, Pets, CIFAR, etc. Specialized – medical and satellite imagery, and Structured – tasks that require geometric understanding like localization.\n",
      "\n",
      "Model Variants.\n",
      "\n",
      "# D.2 TRANSFORMER SHAPE\n",
      "\n",
      "We ran ablations on scaling different dimensions of the Transformer architecture to find out which are best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet for different configurations.\n",
      "\n",
      "In a different line of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-attention in order to be applicable to images.\n",
      "\n",
      "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer.\n",
      "\n",
      "We focus on these two latter datasets as well, but train Transformers instead of ResNet-based models used in prior works.\n",
      "\n",
      "Cosine similarity\n",
      "\n",
      "CONCLUSION\n",
      "\n",
      "Mean attention distance (pixels)\n",
      "We have explored pe direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into pe architecture apart from pe initial patch extraction step.\n",
      "\n",
      "The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
      "\n",
      "Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a).\n",
      "\n",
      "Instead we find that Vision Transformer benefits most from a high resolution (384 × 384) for all tasks.\n",
      "\n",
      "# B.1.2 SELF-SUPERVISION\n",
      "\n",
      "We employ the masked patch prediction objective for preliminary self-supervision experiments.\n",
      "\n",
      "Globally, we find that the model attends to image regions that are semantically relevant for classification (Figure 6).\n",
      "\n",
      "# 4.6 SELF-SUPERVISION\n",
      "\n",
      "Transformers show impressive performance on NLP tasks.\n",
      "\n",
      "We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.\n",
      "\n",
      "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "# Model Architecture\n",
      "\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "\n",
      "Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko|End-to-end object detection with transformers|ECCV, 2020|\n",
      "|Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun|Generative pretraining from pixels|ICML,\n",
      "\u001b[0m> Running step 7359ec5b-d4af-44b1-bd1e-7116a6c896fc. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: {\n",
      "    \"1\": {\n",
      "        \"no\": \"1\",\n",
      "        \"mcq\": \"What is the main advantage of using Transformers in image recognition?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They require less computational resources to train.\",\n",
      "            \"b\": \"They can be trained significantly faster than architectures based on recurrent or convolutional layers.\",\n",
      "            \"c\": \"They can achieve a new state of the art in translation quality.\",\n",
      "            \"d\": \"They can be applied directly to sequences of image patches.\"\n",
      "        },\n",
      "        \"correct\": \"d\"\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"no\": \"2\",\n",
      "        \"mcq\": \"Which of the following is NOT a benefit of using Vision Transformers?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They generally outperform ResNets with the same computational budget.\",\n",
      "            \"b\": \"They can be used for tasks that require geometric understanding.\",\n",
      "            \"c\": \"They can be scaled to very large models.\",\n",
      "            \"d\": \"They can be used for tasks like medical and satellite imagery.\"\n",
      "        },\n",
      "        \"correct\": \"b\"\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"no\": \"3\",\n",
      "        \"mcq\": \"What is the main difference between Vision Transformers and ResNets?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"Vision Transformers use self-attention instead of convolutions.\",\n",
      "            \"b\": \"Vision Transformers are trained on larger datasets.\",\n",
      "            \"c\": \"Vision Transformers have a different architecture.\",\n",
      "            \"d\": \"Vision Transformers are more accurate than ResNets.\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"no\": \"4\",\n",
      "        \"mcq\": \"What is the purpose of the masked patch prediction objective?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"To improve the performance of Vision Transformers on image classification tasks.\",\n",
      "            \"b\": \"To reduce the computational cost of training Vision Transformers.\",\n",
      "            \"c\": \"To make Vision Transformers more robust to noise.\",\n",
      "            \"d\": \"To allow Vision Transformers to be used for tasks like object detection.\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"no\": \"5\",\n",
      "        \"mcq\": \"What is the main advantage of using self-attention in Vision Transformers?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"It allows for significantly more parallelization.\",\n",
      "            \"b\": \"It reduces the computational cost of training.\",\n",
      "            \"c\": \"It improves the accuracy of Vision Transformers.\",\n",
      "            \"d\": \"It makes Vision Transformers more robust to noise.\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"no\": \"6\",\n",
      "        \"mcq\": \"What is the main disadvantage of using Vision Transformers?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They require more computational resources to train than ResNets.\",\n",
      "            \"b\": \"They are not as accurate as ResNets.\",\n",
      "            \"c\": \"They are not as robust to noise as ResNets.\",\n",
      "            \"d\": \"They cannot be used for tasks like object detection.\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"no\": \"7\",\n",
      "        \"mcq\": \"What is the future of Vision Transformers?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They will replace ResNets as the dominant architecture for image recognition.\",\n",
      "            \"b\": \"They will be used for a wider range of tasks, including object detection and segmentation.\",\n",
      "            \"c\": \"They will be used to develop new and innovative applications.\",\n",
      "            \"d\": \"All of the above.\"\n",
      "        },\n",
      "        \"correct\": \"d\"\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"no\": \"8\",\n",
      "        \"mcq\": \"What is the main challenge in using Vision Transformers?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They are difficult to train.\",\n",
      "            \"b\": \"They require large amounts of data.\",\n",
      "            \"c\": \"They are not as accurate as ResNets.\",\n",
      "            \"d\": \"All of the above.\"\n",
      "        },\n",
      "        \"correct\": \"b\"\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"no\": \"9\",\n",
      "        \"mcq\": \"What is the main advantage of using hybrid models?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They improve upon pure Transformers for smaller model sizes.\",\n",
      "            \"b\": \"They are more accurate than pure Transformers.\",\n",
      "            \"c\": \"They are more robust to noise than pure Transformers.\",\n",
      "            \"d\": \"All of the above.\"\n",
      "        },\n",
      "        \"correct\": \"a\"\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"no\": \"10\",\n",
      "        \"mcq\": \"What is the main disadvantage of using hybrid models?\",\n",
      "        \"options\": {\n",
      "            \"a\": \"They are more difficult to train than pure Transformers.\",\n",
      "            \"b\": \"They require more computational resources to train than pure Transformers.\",\n",
      "            \"c\": \"They are not as accurate as pure Transformers.\",\n",
      "            \"d\": \"All of the above.\"\n",
      "        },\n",
      "        \"correct\": \"b\"\n",
      "    }\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "json_file = {\n",
    "            \"number\": 10,\n",
    "            \"topics\": \"Transformers, vision transformers\",\n",
    "            \"difficulty\": \"easy\",\n",
    "            \"response_template\": response_template\n",
    "        }\n",
    "json_str = json.dumps(json_file)\n",
    "\n",
    "# prompt = \"Give me the information about topic transformers and vision transformers\"\n",
    "master.generate_mcq(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
