{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "from pinecone import Pinecone\n",
    "import dotenv \n",
    "import os\n",
    "import nest_asyncio\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "# from utils.parsers import Parser\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from IPython.display import Markdown, display\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index = pc.Index(\"tutor\")\n",
    "index.describe_index_stats()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(delete_all=True, namespace=\"common\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio; nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# encoder = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 5165544e-9026-4366-a660-86e7e3474fd3\n",
      "Started parsing the file under job_id 7e49de96-e99b-43a6-97db-71fa32be26be\n",
      "Started parsing the file under job_id 8d1a4227-b296-4092-8ff6-f6995405ba5b\n",
      "Started parsing the file under job_id 5471f2a7-ea92-4ea2-8d9c-8e107c7c8845\n",
      "..."
     ]
    }
   ],
   "source": [
    "llama_api_key = os.getenv(\"LLAMA_API_KEY\")\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=index,namespace=\"test\")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "parser = LlamaParse(api_key=llama_api_key, result_type=\"markdown\") \n",
    "file_extractor = {\".pdf\": parser,\".csv\":parser,\".docx\":parser,\".txt\":parser}\n",
    "documents = SimpleDirectoryReader(\"backend/artifacts/all\",file_extractor=file_extractor).load_data()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.google import GooglePaLMEmbedding\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "embed_model = GooglePaLMEmbedding(model_name=\"models/embedding-gecko-001\", api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_model = resolve_embed_model(\"local:BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GooglePaLMEmbedding(model_name='models/embedding-gecko-001', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x0000023706D8E6E0>, num_workers=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='671dbb25-4b44-4d86-9cb6-32e7650e60d1', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\movies.csv', 'file_name': 'movies.csv', 'file_type': 'text/csv', 'file_size': 891, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='|movie_id|title                                      |industry |release_year|imdb_rating|studio            |language_id|budget       |revenue      |unit         |currency     |\\n|--------|-------------------------------------------|---------|------------|-----------|------------------|-----------|-------------|-------------|-------------|-------------|\\n|101     |K.G.F: Chapter 2                           |Bollywood|2022        |8.4        |Hombale Films     |3          |1            |12.5         |Billions     |INR          |\\n|102     |Doctor Strange in the Multiverse of Madness|Hollywood|2022        |7          |Marvel Studios    |5          |200          |954.8        |Millions     |USD          |\\n|103     |Thor: The Dark World                       |Hollywood|2013        |6.8        |Marvel Studios    |5          |165          |644.8        |Millions     |USD          |\\n|104     |Thor: Ragnarok                             |Hollywood|2017        |7.9        |Marvel Studios    |5          |180          |854          |Millions     |USD          |\\n|105     |Thor: Love and Thunder                     |Hollywood|2022        |6.8        |Marvel Studios    |5          |250          |670          |Millions     |USD          |\\n|106     |Sholay                                     |Bollywood|1975        |8.1        |United Producers  |1          |Not Available|Not Available|Not Available|Not Available|\\n|107     |Dilwale Dulhania Le Jayenge                |Bollywood|1995        |8          |Yash Raj Films    |1          |400          |2000         |Millions     |INR          |\\n|108     |3 Idiots                                   |Bollywood|2009        |8.4        |Vinod Chopra Films|1          |550          |4000         |Millions     |INR          |\\n|109     |Kabhi Khushi Kabhie Gham                   |Bollywood|2001        |7.4        |Dharma Productions|1          |390          |1360         |Millions     |INR          |\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='69467a87-f5b5-4244-a021-b22fb7514e06', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\nvda_news_1.txt', 'file_name': 'nvda_news_1.txt', 'file_type': 'text/plain', 'file_size': 4675, 'creation_date': '2024-07-06', 'last_modified_date': '2024-06-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='The stock of NVIDIA Corp (NASDAQ:NVDA) experienced a daily loss of ‒3.56% and a 3‒month gain of 32.35%. With an Earnings Per Share (EPS) (EPS) of $1.92, the question arises: is the stock significantly overvalued? This article aims to provide a detailed valuation analysis of NVIDIA, offering insights into its financial strength, profitability, growth, and more. We invite you to delve into this comprehensive analysis.\\n\\n# Company Overview\\n\\nWarning! GuruFocus has detected 10 Warning Signs with NVDA. Click here to check it out.\\n\\nNVDA 30‒Year Financial Data\\n\\nThe intrinsic value of NVDA\\n\\nNVIDIA Corp (NASDAQ:NVDA) is a leading designer of discrete graphics processing units that enhance the experience on computing platforms. The firm′s chips are widely used in various end markets, including PC gaming and data centers. In recent years, NVIDIA has broadened its focus from traditional PC graphics applications such as gaming to more complex and favorable opportunities, including artificial intelligence and autonomous driving, leveraging the high‒performance capabilities of its products.\\n\\nCurrently, NVIDIA′s stock price stands at $418.01, significantly higher than the GF Value of $310.28, indicating the stock might be overvalued. With a market cap of $1 trillion, the valuation seems steep. The following analysis aims to delve deeper into the company′s value.\\n\\n# Is NVIDIA′s Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\n\\nUnderstanding the GF Value\\n\\nThe GF Value is a unique measure of the intrinsic value of a stock, calculated based on historical trading multiples, a GuruFocus adjustment factor, and future business performance estimates. If the stock price is significantly above the GF Value Line, it is overvalued, and its future return is likely to be poor. Conversely, if it is significantly below the GF Value Line, its future return will likely be higher.\\n\\nAccording to GuruFocus Value calculation, NVIDIA (NASDAQ:NVDA) appears to be significantly overvalued. The stock′s current price of $418.01 per share and the market cap of $1 trillion further strengthen this assumption.\\n\\nGiven that NVIDIA is significantly overvalued, the long‒term return of its stock is likely to be much lower than its future business growth.\\n\\n# Link: These companies may deliver higher future returns at reduced risk.\\n\\n# Financial Strength of NVIDIA\\n\\nExamining the financial strength of a company is crucial before investing in its stock. Companies with poor financial strength pose a higher risk of permanent loss. NVIDIA′s cash‒to‒debt ratio of 1.27 is worse than 58.04% of companies in the Semiconductors industry. However, NVIDIA′s overall financial strength is 8 out of 10, indicating a strong financial position.\\n\\n# Profitability and Growth\\n\\nConsistent profitability over the long term reduces the risk for investors. NVIDIA, with its profitability ranking of 10 out of 10, has been profitable for the past 10 years. The company′s operating margin of 17.37% ranks better than 76.5% of companies in the Semiconductors industry.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c184a37a-902a-4b42-adb3-e6442650c81f', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\nvda_news_1.txt', 'file_name': 'nvda_news_1.txt', 'file_type': 'text/plain', 'file_size': 4675, 'creation_date': '2024-07-06', 'last_modified_date': '2024-06-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"However, growth is a crucial factor in a company's valuation. NVIDIA's growth ranks worse than 52.99% of companies in the Semiconductors industry, with its 3-year average revenue growth rate better than 87.88% of companies in the industry.\\n\\nROIC vs WACC\\n\\nComparing a company's return on invested capital (ROIC) to its weighted average cost of capital (WACC) is an effective way to evaluate its profitability. Over the past 12 months, NVIDIA's ROIC was 20.32 while its WACC was 16.74, suggesting that the company is creating value for its shareholders.\\n\\nIs NVIDIA's Stock Significantly Overvalued? A Comprehensive Valuation Analysis\\n\\nConclusion\\n\\nIn conclusion, NVIDIA (NASDAQ:NVDA) appears to be significantly overvalued. Despite its strong financial condition and profitability, its growth ranks lower than 52.99% of companies in the Semiconductors industry. To learn more about NVIDIA stock, you can check out its 30-Year Financials here.\\n\\nTo find out the high-quality companies that may deliver above-average returns, please check out GuruFocus High Quality Low Capex Screener.\\n\\nThis article first appeared on GuruFocus.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='33f7a20a-913d-4954-83aa-a7958e1baa95', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\sample_text.csv', 'file_name': 'sample_text.csv', 'file_type': 'text/csv', 'file_size': 525, 'creation_date': '2024-07-06', 'last_modified_date': '2024-06-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='|text                                                                               |category|\\n|-----------------------------------------------------------------------------------|--------|\\n|Meditation and yoga can improve mental health                                      |Health  |\\n|Fruits, whole grains and vegetables helps control blood pressure                   |Health  |\\n|These are the latest fashion trends for this week                                  |Fashion |\\n|Vibrant color jeans for male are becoming a trend                                  |Fashion |\\n|The concert starts at 7 PM tonight                                                 |Event   |\\n|Navaratri dandiya program at Expo center in Mumbai this october                    |Event   |\\n|Exciting vacation destinations for your next trip                                  |Travel  |\\n|Maldives and Srilanka are gaining popularity in terms of low budget vacation places|Travel  |\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8ba98afd-ac5b-4786-a51d-d1426cf4d7ba', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# International Journal of Information Studies and Libraries\\n\\n8 (2) 2023, 08-18\\n\\nhttp://publishingindia.com/ijisl/\\n\\nGlobal Research Trends on Chatbot and ChatGPT in the 21st Century\\n\\nKunjan Prasad Gupta*, Manendra Kumar Singh**, Shyambali Kumar***\\n\\nAbstract\\n\\nThe study offers a comprehensive analysis of global research on Chatbot and ChatGPT from 2002 to 2023. A rapid research growth has been noted after the year 2017. The research growth, authorship analysis, keyword analysis, citation pattern, co-occurrence of keywords and thematic analysis were analyzed. The USA leads in publications and citations, while Norway achieved the highest average citations. Keyword trends highlighted “Chatbot,” “Artificial intelligence” and “ChatGPT” as research domains. Thematic clusters emerged from co-occurrence analysis. Finally, this study maps the dynamic evolution of Chatbot and ChatGPT research in the first two decades of the 21st century and offers insights for researchers, practitioners and policymakers in the AI research field.\\n\\nKeywords: Bibliometrics, Chatbot, ChatGPT, Lotka Law, Artificial Intelligence, Cluster Analysis\\n\\nIntroduction\\n\\nArtificial Intelligence (AI) has become more prominent in many domains of human existence, exerting a progressively substantial influence that shows no signs of abating. The technology in question finds use in several disciplines, such as machine learning, deep learning, machine cognition, neural networks and natural language processing (Jimma, 2023). AI has many advantages, yet it also gives rise to ethical and social considerations, including the potential displacement of jobs, algorithmic prejudice and concerns around privacy. The continuous area of attention for academics, governments and society involves balancing the beneficial influence of AI and the issues it presents. It is conceivable that the development of a system like human intellect may be achievable in the future.\\n\\nA Chatbot, as defined by Haristiani et al. (2019), is a computer program or AI system that engages in voice or text-based conversations. It serves as an automated conversational agent, facilitating interactive communication between users and the bot. These bots leverage AI and NLP to understand human information and context, aiming to guide users to their desired outcomes with minimal effort (Panda & Chakravarty, 2022). The Chatbot industry has seen significant growth, especially in cloud-based services, driven by recent advancements.\\n\\nAccording to Gupta et al. (2020), ChatGPT, which stands for Chat Generative Pre-Trained Transformer, is a sophisticated Chatbot powered by AI. It was developed by OpenAI and officially released on 30th November, 2022. ChatGPT generates original content in response to both simple and complex prompts (Panda & Kaur, 2023). ChatGPT allows users to shape and guide conversations according to their preferences regarding length, format, style, level of detail and language (Lock, 2022).\\n\\nLiterature Review\\n\\nThe provided literature review discusses various studies related to AI and its applications in different fields.\\n\\nKhosravi et al. (2023) conducted a study on “Chatbots and ChatGPT” and analyzed the scientific literature on Chatbots and ChatGPT. The annual growth rate of literature indicates the tide of research is roughly 19–\\n\\n* Librarian, Govt. Girls College Waidhan, Singrauli, Madhya Pradesh, India. Email: kunjangupta0@gmail.com\\n** Assistant Professor, Department of Library Information Science, Mizoram University, Aizawl, Mizoram, India. Email: manendra@mzu.edu.in\\n*** Librarian, Govt. College Jaipari, Anuppur, Madhya Pradesh, India. Email: shyam.bali044@gmail.com', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='46973cc0-3fd1-4548-9700-b3305099f584', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Global Research Trends on Chatbot and ChatGPT in the 21st Century\\n\\n27% yearly application from COVID-19 and Ontology is categorized into 10 clusters, and the author generalized research turn to mental health and task analysis. Bawack et al. (2022) examine the uses of AI in E-Commerce through bibliometric methods. The finding indicates that China and the USA are leading countries using AI for E-commerce, and recommender systems are the most emerging technology. The study shows that optimization, sentiment analysis and AI-related technologies are the main themes of research. Jimma Bahiru Legesse (2023) explores the uses of AI in healthcare. Scopus database was used for data download, and a drastic change has been noted after 2012 for research growth. A total of nine countries published 96.85% of publications, and the USA was the leading country with 41.84% of literature. The keyword analysis indicates that machine learning, electronic health records and natural language processing were the most frequently used keywords. The uses of AI noted for COVID-19, diabetes, mental health, asthma, dementia and cancer treatment and data management. Xu, D. (2022) analyzed the uses of AI for biotechnology and applied microbiology research. The study focused on quantitative, qualitative and modeling analyses of the literature. The result demonstrates that 128 countries are associated with the research; the USA is the most productive country, and the Chinese Academy of Science is the leading research institution among 584 institutions. Ho and Wang (2020) examine the published literature on AI from Science Citation Index Expanded (SCI-EEPANED). Findings indicate that the USA leads in single authorship, international collaboration, and average citation. Chinese Academy of Science, Islamic Azad University and Massachusetts Institute of Technology (MIT) were the most productive institutions. The USA, Canada and Switzerland are the most collaborative countries in international collaboration. In cluster analysis, neural networks, learning and prediction are prolific keywords. Lucena et al. (2019) investigate AI in higher education. The finding indicates that research in AI has increased worldwide at a rapid pace. B. C. Biswas (2007) examined the research on Botany from 1994 to 2003. The results demonstrate that multiple authorship dominates on single authorship. With 59%, citation books secure first positions, followed by articles 41%. The USA is the most productive country. The average number of 38 citations per article is found. The average length of papers is 11.45 pages. Most of the articles have been published by academic institutions. The keyword analysis\\n\\n# Research Objectives\\n\\n- To examine the Annual Growth Rate of Publication and average citation per year.\\n- To relocate the most relevant source.\\n- To analyze the most productive authors.\\n- To explore the most productive countries and current research trends.\\n\\n# Methodology\\n\\nThe bibliometric method was applied to investigate research trends on Chatbot and ChatGPT. The bibliometric method is used for performance analysis, scientific mapping and developing subjects and prolific authors, institutions, nations, organizations and journals. To explore the research trend, an analysis of keywords, citation patterns and thematic analysis has been performed (Shollapur, 2023). The data were extracted from the Scopus database on 18th August 2023 using the keyword \"Chatbot\" OR \"ChatGPT\" within the article title, abstract and keywords search criteria, and 6478 documents were retrieved and downloaded in CSV files, including all bibliographic information, number of citations, abstract, references, author and title keywords, funding details, etc. MS Excel R software (Biblioshiny) has been used for data analysis and visualization. The information about the essential elements of data available on Chatbot and ChatGPT, like annual scientific Production rate, average citation, etc., is depicted in Table 1. The conference papers', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bc4157ae-2302-4e3f-8336-54c1774a7996', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# International Journal of Information Studies and Libraries\\n\\nVolume 8 Issue 2 July-December 2023\\n\\n|Annual Growth Rate %|43.89|\\n|---|---|\\n|Document Average Age|1.96|\\n|Average citations per doc|7.5|\\n|References|163425|\\n|Keywords Plus (ID)|16124|\\n|Authors|175176478|\\n|Authors of single-authored docs|626|\\n|Document Average Age|1.96|\\n|Single-authored docs|704|\\n|Co-Authors per Doc|3.67|\\n|International co-authorships %|16.84|\\n\\nFigure 1 demonstrates the annual scientific growth of research, which was expanding slowly from 2002 to 2017, not more than 2% annually. After that, the exponential growth rate has been observed to be 32.1% annually. The last five-year growth rate for the year 2019 (613, 9.46%), 2020 (804, 12.41%), 2021 (1091, 16.84%), 2022 (1271, 19.62%) and the highest publication 1862 (32.15%) for the year 2023.\\n\\nThe average citation per year for the journals is depicted in Figure 2. The average citation per year ranges between 0.05 and 6.67, with the lowest 0.05 received for 2002 and the highest 6.67 for 2015, followed by 2017 (5.58) and 2016 (5.54). The highest 2083 citation was noted for 2023, followed by 2022 (1271) and 2021 (1091).\\n\\nAnnual Scientific Production\\n\\nFig. 1: Annual Scientific Growth of Literature', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2223ccd1-93ff-42f2-b8ff-52b3a187b730', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Global Research Trends on Chatbot and ChatGPT in the 21st Century\\n\\nTable 2 demonstrates the ten most relevant sources on Chatbots where lecture notes in Computer Science are noted as the top source with the highest 356 publications and 2228 citations. ACM International Conference Proceeding Series was the second most published source with 220 publications, and the third most common source found CEUR Workshop Proceedings with 168 citations. ACM International Conference on Human Factors in Computing Systems Proceedings has the second highest citation number and highest number of citations per paper, 22.4, more than any regular journals. The sources from computer science and engineering, information science and bioinformatics dominate the research in Chatbots and ChatGPT.\\n\\n|Sr. No.|Sources (Journals, Conferences Etc.)|Articles|Citation|CPP|\\n|---|---|---|---|---|\\n|1|Lecture Notes in Computer Science|356|2228|6.3|\\n|2|ACM International Conference Proceeding Series|210|1003|4.8|\\n|3|CEUR Workshop Proceedings|176|119|1.4|\\n|4|Lecture Notes in Networks and Systems|85|244|0.7|\\n|5|Communications In Computer and Information Science|112|320|2.9|\\n|6|Conference On Human Factors in Computing Systems – Proceedings|93|2080|22.4|\\n|7|Advances In Intelligent Systems and Computing|88|451|5.1|\\n|8|Journal Of Medical Internet Research|86|1674|19.5|\\n|9|Annals Of Biomedical Engineering|68|183|2.7|\\n|10|Lecture Notes in Electrical Engineering|49|67|1.4|\\n\\nThe core journals of any research domain can be identified using Bradford’s law given by Samuel C. Bradford in 1934. The analysis divides all journals into three groups; each represents about one-third of all articles. The research (R software) has been depicted in Figure 3; a total of 2656 journals are identified in this study. First is a core zone, which has 47 (1.769%) journals with 2138 (33%) articles, the second zone is 650 (24.47%) journals with 2203 (34%) articles and the third zone is 1959 (73.75%) journals with 2137 (33%) articles.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b113603-a8c1-4cc3-adf2-3f67e0bf34df', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: Top Ten Highest Productive Authors\\n\\n|Sr. No.|Authors|Articles|\\n|---|---|---|\\n|1|Zhang Y|28|\\n|2|Lee J|27|\\n|3|Li Y|27|\\n|4|Liu Y|27|\\n|5|Denecke K|26|\\n|6|Kim J|25|\\n|7|Zhang J|23|\\n|8|Følstad A|22|\\n|9|Li J|22|\\n|10|Singh S|22|\\n\\nLotka’s Law of Scientific Productivity was given by A. J. Lotka in 1926. It describes the frequency of author publication in a field, which generalises how many articles an author publishes on a particular subject during a specific time frame. It is denoted by Xn Y= Constant, where Y is the frequency of authors making n contributions each (Bensman, S. J., & Smolinsky, L. J. 2017). The analysis is depicted in Fig. 4, where one article was written by 14511 authors, two articles by 1822 authors, three articles by 564 authors, four articles by 247 authors, five articles by 128 authors, six articles by 66 authors, seven articles by 53 authors, eight articles by 33 author, nine articles by 16 authors, 10 articles by 12 authors and 173 articles by more than 13 authors. It indicates that individual research dominates multi-author research patterns.\\n\\nFig. 4: Lotka’s Law of Scientific Productivity', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cfbfb539-e922-4e07-b91a-1f5a8545381f', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"# Global Research Trends on Chatbot and ChatGPT in the 21st Century\\n\\nMost Relevant Affiliations\\n\\nNational University of Singapore wip 48 publications\\nUniversity of Hong Kong wip 48 publications\\nStanford University and pe University of Toronto wip 47 publications each\\nNational Tsing Hua University, Ulster University, and pe University of Auckland wip 43 publications each\\n\\nThe analysis indicates that universities dominate research institutions regarding publications, showing the academic use of AI.\\n\\nMost Relevant Affiliations\\n\\nFigure 5. Top 10 Most Contributing Organizations\\n\\nCitation and publication pattern of countries\\n\\nFig. 5: Figure 5. Top 10 Most Contributing OrganizationsTop 10 Most Contributing Organizations\\n\\nFig. 8 shows the country-wise citation and publication pattern. Analysis indicates the research contribution of 91 countries. USA is leading with 470 Single Country Publications (SCP) and 74(MCP), followed by India 366 (SCP) and 36 (MCP), China 288 (SCP)China 288 (SCP) and 98 (MCP), UK 137 (SCP) and 55\\n\\nMultiple Country Publications (MCP), followed by India 366 (SCP) and 36 (MCP), China 288 (SCP)China 288 (SCP) and 98 (MCP), UK 137 (SCP) and 55 (MCP), Germany 141 (SCP) and 26 (MCP), Italy 133 (SCP)(MCP), Germany 141 (SCP) and 26 (MCP), Italy 133\\n\\nFig. 6 shows the country-wise citation and publicationFig. 8 shows the country-wise citation and publication pattern. Analysis indicates the research\\n\\nand 98 (MCP), UK 137 (SCP) and 55 (MCP), Germany 141 (SCP) and 26 (MCP), Italy 133 (SCP)Corresponding Author's Countries\\n\\nCountries\\n\\nand 31 (MCP), Korea 125 (SCP) and 23 (MCP), Australia 69 (SCP) and 32 (MCP) and Spain 75Australia 69 (SCP) and 32 (MCP) and Spain 75 (SCP)\\n\\nPublications (SCP) and 74 Multiple Country PublicationsMultiple Country Publications (MCP), followed by India 366 (SCP) and 36 (MCP), China 288 (SCP)\\n\\n(SCP) and 23 (MCP). and 23 (MCP).\\n\\nCountries\\n\\nUNITCO KINGOC -GERLANY\\n\\nUTAlT\\n\\nKGRCA\\n\\nAUSTRAL\\n\\nBRADL\\n\\nINDONF34 -\\n\\nNETHERLNDE\\n\\nFTUNcd\\n\\ntoR 4Y\\n\\nDocuments\\n\\nFig. 6: Fig. 6: Top 15 Most Productive CountriesTop 15 Most Productive Countries\\n\\nUSA received pe highest citation 5145 wip 9.50 Citation Per Paper (CPP)\\nChina 2654 wip 6.90 (CPP)\\nUnited Kingdom 2555 wip 13.30 (CPP)\\nKorea 1555 wip 10.5 (CPP)\\nGermany 1425 wip 8.50 (CPP)\\nIndia 1308 wip 3.30\\n\\nFig. 6: Top 15 Most Productive Countries\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e44119c1-10eb-4158-ad7e-0077a8b13894', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# International Journal of Information Studies and Libraries\\n\\nVolume 8 Issue 2 July-December 2023\\n\\n**Table 4: Top 10 Most Cited and Publishing Country**\\n|Sr. No.|Country|Total Publication|Total Citation|CPP|\\n|---|---|---|---|---|\\n|1|USA|1314|15330|11.6|\\n|2|India|934|4642|4.95|\\n|3|United Kingdom|477|5312|11.01|\\n|4|China|474|4216|8.8|\\n|5|Germany|382|2761|7.2|\\n|6|Italy|308|2594|8.8|\\n|7|South Korea|235|2592|11|\\n|8|Australia|216|2592|11.09|\\n|9|Taiwan|199|1507|7.6|\\n|10|Norway|82|1355|16.07|\\n\\nIn 2023, followed by Chatbots, AI and natural language processing in 2022, natural language processing systems, conversational agents and students in 2021, human-computer interaction, user interfaces, semantics in 2020, Chabot, human engineering, intelligent agent in 2019, ubiquitous computing, AI markup language, Turing test in 2018, aim in 2017 and latent semantic analysis in 2016, java programming language, human-computer dialogues, e-learning environment in 2014 and virtual worlds, non-player character in 2010 and mathematical models in 2005. This shows that Chatbot and Chatbot have various research subdomains identified from the beginning of research years, especially the first decade of the 21st century. The second decade is heavily involved in research of semantics, Markup languages, high-performance programming, and task-based systems, later converted into AI and further as AI assistance systems.\\n\\nTrend Topic and Keyword Analysis\\n\\nThe trend topics have been analysed to locate the 20 most emerging issues between the years 2012 and 2023. Fig. 7 indicates that for the year 2023, the keywords ChatGPT occurred (483) times, followed by AI (153) and education (99). Furthermore, in the year 2022, AI (986), machine learning (357) and COVID-19 (151). In the year 2021, the terms Chatbot (2260), Chatbots (605) and natural language processing (502) are the most frequently used terms. Year 2020, noted as question answering (37), dialog system (36) and human-computer interaction (34). In the year 2019, intelligent agents (13), conversation (13) and agent (11). For the year 2018, aim (53), pattern matching (12) and question answering (7). The results of the keyword indicate that human, humans and software most relevant and highest time occurred.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f1839146-c6cc-4122-bfa3-fae4eda58844', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Global Research Trends on Chatbot and ChatGPT in the 21st Century\\n\\nTrend Topics\\n\\nEducation\\nArtificial Intelligence\\nMachine Learning\\nCOVID-19\\nChatbots\\nChatbot\\nNatural Language Processing\\nHuman-Computer Interaction\\nHuman-computer Interaction\\nFrequency\\n\\nFig. 7: Top 20 Year Wise Keywords with Highest Trends\\n\\nCo-Occurrence of Keywords and Thematic Analysis\\n\\nCo-occurrence of Keywords and Thematic Analysis\\n\\nFig. 8 maps the occurrence of keywords and the three clusters identified. Cluster 1 main theme was AI, with sub-themes machine learning, mental health, health care, Chatbot, social media, COVID-19, decision making, etc. The cluster 2 main themes found Humans, including sub-theme humans, article, adults, software, language, communication, interpersonal communication, letter, control study, etc. The main themes Chatbots noted for cluster 3, with sub-themes Chatbot, natural language, language processing, learning algorithms, e-learning, semantics, computational linguistics, knowledge base system, machine learning, behavioural research, social networking (online), speech processing, virtual assistance, dialogue system, sentiment analysis, information use, language model and user experience. The thematic research of co-occurrence keywords indicates that the main themes are not scattered in different areas and cover the utilisation of AI with sophisticated systems for enhancing excellence to support human centric activity.\\n\\n|Cluster 1|Cluster 2|Cluster 3|\\n|---|---|---|\\n|Artificial Intelligence|Chatbots|Natural Language Processing Systems|\\n|AI with sophisticated systems for enhancing human excellence to support human centric activity| | |\\n\\nFig. 8: Co-Occurrence of Keywords', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='fdc4a0fd-7e7d-4cb9-a163-d431fda24a19', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Fig. 8: Co-Occurrence of Keywords\\n\\nFigure 9 indicates the emerging theme for the present study. Two themes have been analysed: the primary and niche themes of Chatbot and ChatGPT. The basic theme represented by Chatbots is found as the central theme with 1809 documents, followed by chatbot (891), natural language system (311), deep learning (301), and human-computer processing system (740), natural language processing (462), conversational agent (452), natural interaction (271) documents. In niche themes, AI with 1022 documents found most relevant, followed by human (768), article (307), adult (200), female (188), health care (179), mental health (176), male (167), and ChatGPT (143). The thematic analysis indicates that AI is mainly used to support human intelligence, not as independent research.\\n\\n|Niche Themes|Hot or Tncmci|\\n|---|---|\\n|artificial intelligence|human|\\n|humans|humans|\\n|chatbot|chatbot|\\n\\nFig. 9: Keywords Thematic Map\\n\\n# Result and Conclusion\\n\\nThe study, encompassing an analysis of 6252 articles on Chatbot and ChatGPT from 2002 to 2023 retrieved from the Scopus database, reveals several noteworthy trends. The annual growth rate of ChatGPT literature stood at 43.12%, exhibiting a prolonged period of growth between 2002 and 2017, followed by a significant exponential surge post-2017. Particularly striking is the substantial increase in publication rates from 2019 to 2023, signifying a heightened interest and research activity within this field. While the average yearly citation rate displayed variations, the year 2023 garnered the highest number of citations, with 1862 citations, closely trailed by 2022 and 2021. In terms of sources, lecture notes in Computer Science (including Subseries Lecture Notes in AI and Bioinformatics) emerged as the most relevant source, with 356 articles. Notably, Weizenbaum J claimed the spotlight as the most pertinent author with 621 citations. On a local impact scale, the Conference on Human Factors in Computing Systems – Proceedings held the top rank with a 25 H-index. Author Zhang Y proved to be the most prolific, boasting 28 publications, followed closely by Lee J and Li Y, each with 27 publications. Regarding impact, author Folstad A claimed the foremost position with an 11 h-index, trailed by Kim S, Kowatsch T, and Lombardi, each with a 10 h-index. Notably, the University of California and Bina Nusantara University secured the leading positions as the top two contributing institutions. On a country level, the USA maintained its top position with 544 publications, closely followed by India with 402 publications. However, a shift occurred concerning citations received, with the USA leading in citation reception with 5145 citations, averaging 9.50 citations per article, followed by China with 2654 citations and an average of 6.90 citations per article. While India held the second position in terms of publications, its ranking dropped to third in citation impact, highlighting the importance of literature quality in attracting research attention. Remarkably, Norway achieved the highest average citations per article, with 28.90, followed by Norway once more with 18.90 citations per article. Lastly, the study’s author keyword analysis spotlighted Chatbot occurring 2303 times, followed by AI (104 times).', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d1661e9e-f50e-4dc9-a3bf-90550067f567', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Global Research Trends on Chatbot and ChatGPT in the 21st Century\\n\\nChatbots (616 times), ChatGPT (563 times) and natural language processing (511 times). In the contemporary landscape, ChatGPT, AI and education emerged as the most trending topics in 2023, while AI machine learning and COVID-19 took center stage in 2022. The author keyword analysis unveiled humans and software as the most trending topics in 2023, whereas chatbots, AI and natural language processing dominated the discourse in 2022. Co-occurrence analysis identified three primary clusters: Artificial intelligence, encompassing machine learning, mental health, and ChatGPT-related documents; Chatbots, covering Chatbots, natural language or deep learning documents; and Humans, delving into software and language-related subjects. In summation, this study effectively fulfilled its objectives and provides valuable insights into the global research trends in Chatbots and ChatGPT, serving as a valuable resource for the research community and policymakers.\\n\\n# References\\n\\nJimma, B. L. (2023). Artificial intelligence in healpcare: A bibliometric analysis. Telematics and Informatics Reports, 100041. Retrieved July 2, 2023.\\nAgrawal, A., Gans, J., & Goldfarb, A. (2022). ChatGPT and how AI disrupts industries. Harvard Business Review. Retrieved July 3, 2023, from https://hbr.org/2022/12/chatgpt-andhow-ai-disrupts-industries\\nAltaf, Y. (2023). 5 ways ChatGPT will impact digital marketing. Entrepreneur. Retrieved July 4, 2023, from https://www.entrepreneur.com/growing-a-business/5-ways-chatgpt-will-impact-digital-marketing/446208\\nBishop, C. M. (1994). Neural networks and peir applications. Review of Scientific Instruments, 65, article 1803. doi:https://doi.org/10.1063/1.1144830\\nBiswas, S. S. (2023). Potential use of ChatGPT in global warming. Annals of Biomedical Engineering, 51, 1126-1127. doi:https://doi.org/10.1007/s10439-023-03171-8\\nBrockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv. Retrieved July 8, 2023; doi:https//doi.org/10.48550/arXiv.1606.01540\\nBudzianowski, P., & Vulić, I. (2019). Hello, it’s GPT-2--how can I help you? towards pe use of pretrained language models for task-oriented dialogue systems.\\nDale, R. (2017). NLP in a post-trup world. Natural Language Engineering, 23(2), 319-324.\\nDale, R. (2021). GPT-3 What’s it good for? Natural Language Engineering, 27(1), 113-118.\\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv. Retrieved July 25, 2023; doi:https://doi.org/10.48550/arXiv.1810.04805\\nDiaz, M. (2023). How to use ChatGPT: Everyping you need to know. ZDNET. Retrieved July 27, 2023.\\nElse, H. (2023). Abstracts written by ChatGPT fool scientists. Nature, 613(7344), 423 doi:https://doi.org/10.1038/d41586-023-00056-7\\nErhan, D., Bengio, Y., Courville, A., Manzagol, P., & Vincent, P. (2010). Why does unsupervised pre-training help deep learning. Journal of Machine Learning Research, 11, 625-660.\\nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, scope, limits, and consequences. Minds and Machines, 30(4), 681-694.\\nGoh, G., Cammarata, N., Voss, C., Carter, S., Petrov, M., Schubert, L., Radford, A., & Olah, C. (2021). Multimodal neurons in artificial neural networks. Retrieved July 31, 2023; doi:https://doi.org/10.23915/distill.00030\\nKing, M. R. (2022). The future of AI in medicine: A perspective from a chatbot. Annals of Biomedical Engineering. Retrieved August 1, 2023; doi:https://doi.org/10.1007/s10439-022-03121-w\\nKirmani, A. R. (2022). Artificial intelligence-enabled science poetry. ACS Energy Letters, 8, 574-576. Retrieved August 1, 2023.\\nLee, C., Panda, P., Srinivasan, G., & Roy, K. (2018). Training deep spiking convolutional neural networks wip STDP-based unsupervised pre-training followed by supervised fine-tuning. Frontiers in Neuroscience, 12, article 435. Retrieved August 7, 2023.\\nLiu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., & Tang, J. (2021). GPT understands, too. arXiv. Retrieved August 1, 2023; doi:https://doi.org/10.48550/arXiv.2103.10385', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a28eca73-d1c1-4750-a210-105e81c90dd0', embedding=None, metadata={'file_path': 'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app\\\\backend\\\\artifacts\\\\all\\\\ssrn-4878177.pdf', 'file_name': 'ssrn-4878177.pdf', 'file_type': 'application/pdf', 'file_size': 1003646, 'creation_date': '2024-07-11', 'last_modified_date': '2024-07-06'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='# Volume 8 Issue 2 July-December 2023\\n\\nPanda, S., & Kaur, N. (2023). Exploring pe viability of ChatGPT as an alternative to traditional ChatBot systems in library and information centers. Library Hi Tech News, 40(3), 22-25. doi:http://dx.doi. org/10.1108/lhtn-02-2023-0032\\nSachdev, S. (2023). ChatGPT and its impact on society. The Times of India. Retrieved August 12, 2023, from https://timesofindia. indiatimes.com/readersblog/marketing-savvy/ chatgpt-and-its-impact-on-society-50445\\nUNESCO. (2021). AI and education: Guidance for policymakers. Retrieved August 15, 2023 from https:// unesdoc.unesco.org/ark:/48223/ pf0000376709\\nUNESCO. (2021). Recommendation on pe epics of artificial intelligence. Retrieved August 20, 2023, from https://unesdoc.unesco.org/ark:/48223/ pf0000381137\\nUNESCO World Commission on pe Epics of Scientific Knowledge and Technology. (2019). Preliminary study on pe epics of artificial intelligence. Retrieved August 20, 2023, from https:// unesdoc.unesco.org/ ark:/48223/pf0000367823\\nBensman, S. J., & Smolinsky, L. J. (2017). Lotka’s inverse square law of scientific productivity: Its mepods and statistics. Journal of pe Association for Information Science and Technology, 68(7), 1786-1791. Retrieved August 25, 2023.\\n\\n# International Journal of Information Studies and Libraries\\n\\nLucy, L., & Bamman, D. (2021). Gender and representation bias in GPT-3 generated stories. Proceedings of pe Workshop on Narrative Understanding, 3, 48-55.\\nLund, B. D., & Wang, T. (2023). Chatting about ChatGPT: How may AI and GPT impact academia and libraries? Library Hi Tech News, 40(3), 26-29. doi:https://doi. org/10.1108/lhtn-01-2023-0009\\nMandelaro, J. (2023, February 27). How will AI chatbots like ChatGPT affect higher education? News Center. Retrieved August 7, 2023.\\nMok, A., & Zinkula, J. (2023, February 2). ChatGPT may be coming for our jobs. Here are pe 10 roles pat AI is most likely to replace. Business Insider Africa. Retrieved August 7, 2023, from https://africa.businessinsider.com/ news/chatgpt-may-becoming-for-our-jobs- here-are-pe-10-roles-pat-ai-ismost-likely-to/ grmgtk3\\nQ.ai - Powering a Personal Wealp Movement. (2023). What is ChatGPT? How AI is transforming multiple industries. Forbes. Retrieved August 8, 2023, from https://www.forbes.com/sites/qai/2023/02/01/what- ischatgpt-how-ai-is-transforming-multipleindustries /?sh=64e915ce728e\\nPanda, S., & Chakravarty, R. (2022). Adapting intelligent information services in libraries: A case of smart AI chatbots. Library Hi Tech News, 39(1), 12-15. doi:https://doi.org/10.1108/lhtn-11-2021-0081', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "PineconeApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Fri, 12 Jul 2024 11:32:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1742', 'x-pinecone-request-id': '2530441891089373319', 'x-envoy-upstream-service-time': '39', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 1024\",\"details\":[]}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPineconeApiException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\base.py:145\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, service_context, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m    138\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    139\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     transformations,\n\u001b[0;32m    141\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    143\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    146\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    147\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m    148\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m    149\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    150\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m    151\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    153\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     78\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m     79\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     80\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m     81\u001b[0m     objects\u001b[38;5;241m=\u001b[39mobjects,\n\u001b[0;32m     82\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m     83\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     85\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_index_from_nodes(\n\u001b[0;32m     95\u001b[0m         nodes \u001b[38;5;241m+\u001b[39m objects, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:308\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    301\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    302\u001b[0m ):\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m     )\n\u001b[1;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index_from_nodes(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:280\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    278\u001b[0m     run_async_tasks(tasks)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_nodes_to_index(\n\u001b[0;32m    281\u001b[0m         index_struct,\n\u001b[0;32m    282\u001b[0m         nodes,\n\u001b[0;32m    283\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress,\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs,\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:234\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[1;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m    233\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node_with_embedding(nodes_batch, show_progress)\n\u001b[1;32m--> 234\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node, new_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nodes_batch, new_ids):\n\u001b[0;32m    240\u001b[0m             \u001b[38;5;66;03m# NOTE: remove embedding from node to avoid duplication\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\llama_index\\vector_stores\\pinecone\\base.py:393\u001b[0m, in \u001b[0;36mPineconeVectorStore.add\u001b[1;34m(self, nodes, **add_kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(node_id)\n\u001b[0;32m    392\u001b[0m     entries\u001b[38;5;241m.\u001b[39mappend(entry)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pinecone_index\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    394\u001b[0m     entries,\n\u001b[0;32m    395\u001b[0m     namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespace,\n\u001b[0;32m    396\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minsert_kwargs,\n\u001b[0;32m    398\u001b[0m )\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\utils\\error_handling.py:10\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ProtocolError):\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\index.py:176\u001b[0m, in \u001b[0;36mIndex.upsert\u001b[1;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m total_upserted \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vectors), batch_size):\n\u001b[1;32m--> 176\u001b[0m     batch_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upsert_batch(vectors[i : i \u001b[38;5;241m+\u001b[39m batch_size], namespace, _check_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    177\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(batch_result\u001b[38;5;241m.\u001b[39mupserted_count)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# we can't use here pbar.n for the case show_progress=False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\index.py:189\u001b[0m, in \u001b[0;36mIndex._upsert_batch\u001b[1;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespace)])\n\u001b[0;32m    187\u001b[0m vec_builder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_api\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    190\u001b[0m     UpsertRequest(\n\u001b[0;32m    191\u001b[0m         vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(vec_builder, vectors)),\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict,\n\u001b[0;32m    193\u001b[0m         _check_type\u001b[38;5;241m=\u001b[39m_check_type,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    195\u001b[0m     ),\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    197\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\api_client.py:772\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    762\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" This method is invoked when endpoints are called\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m \n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\api\\data_plane_api.py:1084\u001b[0m, in \u001b[0;36mDataPlaneApi.__init__.<locals>.__upsert\u001b[1;34m(self, upsert_request, **kwargs)\u001b[0m\n\u001b[0;32m   1081\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_host_index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1082\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupsert_request\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m   1083\u001b[0m     upsert_request\n\u001b[1;32m-> 1084\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_with_http_info(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\api_client.py:834\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    830\u001b[0m     header_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_client\u001b[38;5;241m.\u001b[39mselect_header_content_type(\n\u001b[0;32m    831\u001b[0m         content_type_headers_list)\n\u001b[0;32m    832\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m header_list\n\u001b[1;32m--> 834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mendpoint_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp_method\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mform\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresponse_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_check_return_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcollection_format\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\api_client.py:409\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;124;03m    then the method will return the response directly.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m                           \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api, (resource_path,\n\u001b[0;32m    418\u001b[0m                                                method, path_params,\n\u001b[0;32m    419\u001b[0m                                                query_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    427\u001b[0m                                                _request_timeout,\n\u001b[0;32m    428\u001b[0m                                                _host, _check_type))\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\api_client.py:203\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n\u001b[0;32m    207\u001b[0m return_data \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\api_client.py:196\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    192\u001b[0m     url \u001b[38;5;241m=\u001b[39m _host \u001b[38;5;241m+\u001b[39m resource_path\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\api_client.py:455\u001b[0m, in \u001b[0;36mApiClient.request\u001b[1;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(url,\n\u001b[0;32m    448\u001b[0m                                     query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m    449\u001b[0m                                     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m                                     _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[0;32m    453\u001b[0m                                     body\u001b[38;5;241m=\u001b[39mbody)\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(url,\n\u001b[0;32m    464\u001b[0m                                 query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m    465\u001b[0m                                 headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m                                 _request_timeout\u001b[38;5;241m=\u001b[39m_request_timeout,\n\u001b[0;32m    469\u001b[0m                                 body\u001b[38;5;241m=\u001b[39mbody)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\rest.py:302\u001b[0m, in \u001b[0;36mRESTClientObject.POST\u001b[1;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPOST\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, query_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, post_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m          body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\core\\client\\rest.py:261\u001b[0m, in \u001b[0;36mRESTClientObject.request\u001b[1;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m599\u001b[39m:\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[1;31mPineconeApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Fri, 12 Jul 2024 11:32:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '103', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '1742', 'x-pinecone-request-id': '2530441891089373319', 'x-envoy-upstream-service-time': '39', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Vector dimension 768 does not match the dimension of the index 1024\",\"details\":[]}\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "        documents, \n",
    "        storage_context=storage_context,\n",
    "        embed_model=embed_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class parseFormat(BaseModel):\n",
    "    number: int\n",
    "    subject: str\n",
    "    tone: str\n",
    "\n",
    "\n",
    "mcq_template=\"\"\"\n",
    "You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_template}\n",
    "\n",
    "\"\"\"\n",
    "response_template = \"your response template\"\n",
    "\n",
    "json_string = {\n",
    "            \"number\": 10,\n",
    "            \"subject\": \"Maths\",\n",
    "            \"tone\": \"easy\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# parser = PydanticOutputParser(parseFormat)\n",
    "# json_prompt_str = parser.format(mcq_template)   # code_parser_template will be filled with output data of parser.\n",
    "# prompt_template = PromptTemplate(json_prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class parseFormat(BaseModel):\n",
    "    number: int\n",
    "    subject: str\n",
    "    tone: str\n",
    "\n",
    "\n",
    "mcq_template=\"\"\"\n",
    "You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_template}\n",
    "\n",
    "\"\"\"\n",
    "response_template = \"your response template\"\n",
    "\n",
    "json_string = {\n",
    "            \"number\": 10,\n",
    "            \"subject\": \"Maths\",\n",
    "            \"tone\": \"easy\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# parser = PydanticOutputParser(parseFormat)\n",
    "# json_prompt_str = parser.format(mcq_template)   # code_parser_template will be filled with output data of parser.\n",
    "# prompt_template = PromptTemplate(json_prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class parseFormat(BaseModel):\n",
    "    number: int\n",
    "    subject: str\n",
    "    tone: str\n",
    "\n",
    "\n",
    "mcq_template=\"\"\"\n",
    "You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_template}\n",
    "\n",
    "\"\"\"\n",
    "response_template = \"your response template\"\n",
    "\n",
    "json_string = {\n",
    "            \"number\": 10,\n",
    "            \"subject\": \"Maths\",\n",
    "            \"tone\": \"easy\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# parser = PydanticOutputParser(parseFormat)\n",
    "# json_prompt_str = parser.format(mcq_template)   # code_parser_template will be filled with output data of parser.\n",
    "# prompt_template = PromptTemplate(json_prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class parseFormat(BaseModel):\n",
    "    number: int\n",
    "    subject: str\n",
    "    tone: str\n",
    "\n",
    "\n",
    "mcq_template=\"\"\"\n",
    "You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_template}\n",
    "\n",
    "\"\"\"\n",
    "response_template = \"your response template\"\n",
    "\n",
    "json_string = {\n",
    "            \"number\": 10,\n",
    "            \"subject\": \"Maths\",\n",
    "            \"tone\": \"easy\"\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# parser = PydanticOutputParser(parseFormat)\n",
    "# json_prompt_str = parser.format(mcq_template)   # code_parser_template will be filled with output data of parser.\n",
    "# prompt_template = PromptTemplate(json_prompt_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "\n",
    "class parseFormat(BaseModel):\n",
    "    number: int\n",
    "    subject: str\n",
    "    tone: str\n",
    "    response_template: str\n",
    "\n",
    "\n",
    "mcq_template=\"\"\"\n",
    "You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_template}\n",
    "\n",
    "\"\"\"\n",
    "response_template = \"your response template\"\n",
    "\n",
    "json_string = {\n",
    "            \"number\": 10,\n",
    "            \"subject\": \"Maths\",\n",
    "            \"tone\": \"easy\",\n",
    "            \"response_template\": response_template\n",
    "        }\n",
    "json_str = json.dumps(json_string)\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(parseFormat)\n",
    "json_parsed_data = parser.parse(json_str)\n",
    "formatted_prompt = mcq_template.format(**json_parsed_data.dict())\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(template=formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=[], template='\\nYou are an expert MCQ maker. Use the information of the study material given by the user, it is your job to create a quiz  of 10 multiple choice questions for Maths students in easy tone. \\nMake sure the questions are not repeated and check all the questions to be conforming the text as well.\\nMake sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 10 MCQs\\n### RESPONSE_JSON\\nyour response template\\n\\n')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Model.__init__() got an unexpected keyword argument 'max_temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m genai\u001b[38;5;241m.\u001b[39mlist_models():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerateContent\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39msupported_generation_methods:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\google\\generativeai\\models.py:206\u001b[0m, in \u001b[0;36mlist_models\u001b[1;34m(page_size, client, request_options)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mlist_models(page_size\u001b[38;5;241m=\u001b[39mpage_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options):\n\u001b[0;32m    205\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39mto_dict(model)\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_types\u001b[38;5;241m.\u001b[39mModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel)\n",
      "\u001b[1;31mTypeError\u001b[0m: Model.__init__() got an unexpected keyword argument 'max_temperature'"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The meaning of life is a question that has been pondered by philosophers and theologians for centuries, and there is no single, universally accepted answer.  \\n\\nHere's a breakdown of some common perspectives:\\n\\n**Philosophical Perspectives:**\\n\\n* **Nihilism:**  Life has no inherent meaning or purpose. It is up to the individual to create their own meaning.\\n* **Existentialism:**  The individual is responsible for creating their own meaning and purpose in a world that is inherently meaningless.\\n* **Absurdism:**  The universe is indifferent to human existence, and the search for meaning is futile.  Embrace the absurdity of life and find joy in the present moment.\\n* **Hedonism:**  The goal of life is to maximize pleasure and minimize pain.\\n* **Utilitarianism:**  The meaning of life is to maximize happiness for the greatest number of people.\\n\\n**Religious Perspectives:**\\n\\n* **Monotheistic Religions:**  The meaning of life is to serve a higher power, often through worship, prayer, and good works.\\n* **Buddhism:**  The meaning of life is to achieve enlightenment and break free from the cycle of suffering.\\n* **Hinduism:**  The meaning of life is to achieve liberation (moksha) from the cycle of birth, death, and rebirth.\\n\\n**Personal Perspectives:**\\n\\nUltimately, the meaning of life is a personal question that each individual must answer for themselves. Some common themes include:\\n\\n* **Love and connection:** Finding love and building meaningful relationships with others.\\n* **Purpose and fulfillment:** Finding a sense of purpose and making a positive impact on the world.\\n* **Growth and learning:** Continuously learning and developing as a person.\\n* **Creativity and expression:** Finding ways to express oneself creatively.\\n* **Happiness and well-being:** Seeking happiness, contentment, and a sense of peace.\\n\\n**It's important to note that:**\\n\\n* There is no right or wrong answer to the question of the meaning of life.\\n* Your understanding of the meaning of life may change and evolve over time.\\n* The meaning of life can be a source of comfort, inspiration, and motivation.\\n\\nUltimately, the search for meaning is an ongoing journey. There is no single destination, but the exploration itself can be enriching and fulfilling. \\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 8,\n",
       "        \"candidates_token_count\": 477,\n",
       "        \"total_token_count\": 485\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Model.__init__() got an unexpected keyword argument 'max_temperature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerativeai\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m genai\u001b[38;5;241m.\u001b[39mlist_models():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerateContent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39msupported_generation_methods:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\google\\generativeai\\models.py:206\u001b[0m, in \u001b[0;36mlist_models\u001b[1;34m(page_size, client, request_options)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mlist_models(page_size\u001b[38;5;241m=\u001b[39mpage_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options):\n\u001b[0;32m    205\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39mto_dict(model)\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_types\u001b[38;5;241m.\u001b[39mModel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel)\n",
      "\u001b[1;31mTypeError\u001b[0m: Model.__init__() got an unexpected keyword argument 'max_temperature'"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "for m in genai.list_models():\n",
    "    if \"generateContent\" in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "llm = Gemini(model=\"models/gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "from langchain.prompts import PromptTemplate\n",
    "from llama_index.core.agent import ReActAgent\n",
    "import json\n",
    "\n",
    "class parseFormat(BaseModel):\n",
    "    number: int\n",
    "    subject: str\n",
    "    tone: str\n",
    "    response_template: str\n",
    "\n",
    "QnA_agent_context = \"\"\"Purpose: The primary role of this agent is to assist users to make questions and answers on the basis of the given study material.\"\"\"\n",
    "mcq_template=\"\"\"\n",
    "You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_template}\n",
    "\n",
    "\"\"\"\n",
    "response_template = \"your response template\"\n",
    "tools = []\n",
    "json_string = {\n",
    "            \"number\": 10,\n",
    "            \"subject\": \"Maths\",\n",
    "            \"tone\": \"easy\",\n",
    "            \"response_template\": response_template\n",
    "        }\n",
    "json_str = json.dumps(json_string)\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(parseFormat)\n",
    "json_parsed_data = parser.parse(json_str)\n",
    "formatted_prompt = mcq_template.format(**json_parsed_data.dict())\n",
    "\n",
    "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True, context=QnA_agent_context)\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step c4147d7f-7554-4f29-a499-ed25a0227e65. Step input: \n",
      "You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to create a quiz  of 10 multiple choice questions for Maths students in easy tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 10 MCQs\n",
      "### RESPONSE_JSON\n",
      "your response template\n",
      "\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: english. I need to use a tool to help me answer the question.\n",
      "Action: question_generator\n",
      "Action Input: {'context': 'You are an expert MCQ maker. Use the information of the study material given by the user, it is your job to create a quiz  of 10 multiple choice questions for Maths students in easy tone.\\nMake sure the questions are not repeated and check all the questions to be conforming the text as well.', 'num_questions': 10, 'difficulty': 'easy', 'subject': 'Maths'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `question_generator`.\n",
      "\u001b[0m> Running step 9c6956e6-6efa-4c7c-9fd2-96a379f83cca. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I apologize for the error. I will try to answer the question without using any tools.\n",
      "Answer: I am sorry, I cannot create a quiz of 10 multiple choice questions for Maths students in easy tone without the study material.\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='I am sorry, I cannot create a quiz of 10 multiple choice questions for Maths students in easy tone without the study material.', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.query(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
