{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import google.generativeai as genai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading C:/vidit/codes/PROJECTS/Tutor-app/backend/knowledge_store/transformer_research_paper.pdf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\langchain_community\\document_loaders\\text.py:43\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 43\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 322\u001b[0m (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8f in position 10: invalid start byte",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[0;32m      3\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/vidit/codes/PROJECTS/Tutor-app/backend/knowledge_store/transformer_research_paper.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2500\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\langchain_core\\document_loaders\\base.py:30\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\langchain_community\\document_loaders\\text.py:56\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading C:/vidit/codes/PROJECTS/Tutor-app/backend/knowledge_store/transformer_research_paper.pdf"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(r\"C:/vidit/codes/PROJECTS/Tutor-app/backend/knowledge_store/transformer_research_paper.pdf\")\n",
    "data = loader.load()\n",
    "print(data[0].page_content[0:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_id: 102\n",
      "title: Doctor Strange in the Multiverse of Madness\n",
      "industry: Hollywood\n",
      "release_year: 2022\n",
      "imdb_rating: 7\n",
      "studio: Marvel Studios\n",
      "language_id: 5\n",
      "budget: 200\n",
      "revenue: 954.8\n",
      "unit: Millions\n",
      "currency: USD\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(\"C:/vidit/codes/PROJECTS/Tutor-app/artifacts/movies.csv\")\n",
    "data = loader.load()\n",
    "print(data[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "The study offers a comprehensive analysis of global \n",
      "research on Chatbot and ChatGPT from 2002 to 2023. \n",
      "A rapid research growth has been noted after the \n",
      "year 2017. The research growth, authorship analysis, \n",
      "keyword analysis, citation pattern, co-occurrence \n",
      "of keywords and thematic analysis were analysed. \n",
      "The USA leads in publications and citations, while \n",
      "the Norway achieved the highest average citations. \n",
      "Keyword trends highlighted “Chatbot,” “Artificial \n",
      "intelligence” and “ChatGPT” as research domains. \n",
      "Thematic clusters emerged from co-occurrence \n",
      "analysis. Finally, this study maps the dynamic \n",
      "evolution of Chatbot and ChatGPT research in the first \n",
      "two decades of the 21st century and offers insights for \n",
      "researchers, practitioners and policymakers in the AI \n",
      "research field. \n",
      "Keywords:  Bibliometrics, Chabot, ChatGPT, Lotka \n",
      "Law, Artificial Intelligence, Cluster AnalysisGlobal Research Trends on Chatbot and ChatGPT \n",
      "in the 21st Century\n",
      "Kunjan Prasad Gupta*, Manendra Kumar Singh**, Shyambali Kumar***\n",
      "Introduction\n",
      "Artificial Intelligence (AI) has become more prominent \n",
      "in many domains of human existence, exerting a \n",
      "progressively substantial influence that shows no signs of \n",
      "abating. The technology in question finds use in several \n",
      "disciplines, such as machine learning, deep learning, \n",
      "machine cognition, neural networks and natural language \n",
      "processing (Jimma, 2023). AI has many advantages, yet \n",
      "it also gives rise to ethical and social considerations, \n",
      "including the potential displacement of jobs, algorithmic \n",
      "prejudice and concerns around privacy. The continuous \n",
      "area of attention for academics, governments and society \n",
      " * Librarian, Govt. Girls College Waidhan, Singrauli, Madhya Pradesh, India. Email: kunjangupta0@gmail.com \n",
      " ** Assistant Professor, Department of Library  Information Science, Mizoram University, Aizawl, Mizoram, India.  \n",
      "Email: manendra@mzu.edu.in\n",
      "***  Librarian, Govt. College Jaithari, Anuppur, Madhya Pradesh, India. Email: shyam.bali044@gmail.com\n",
      "  International Journal of Information Studies and Libraries\n",
      "8 (2) 2023, 08-18\n",
      "http://publishingindia.com/ijisl/\n",
      "involves balancing the beneficial influence of AI and the \n",
      "issues it presents. It is conceivable that the development \n",
      "of a system like human intellect may be achievable in the \n",
      "future. \n",
      "A Chatbot, as defined by Haristiani et al. (2019), is \n",
      "a computer programme or AI system that engages \n",
      "in voice or text-based conversations. It serves as \n",
      "an automated conversational agent, facilitating  \n",
      "interactive communication between users and the bot. \n",
      "These bots leverage AI and NLP to understand human \n",
      "information and context, aiming to guide users to \n",
      "their desired outcomes with minimal effort (Panda \n",
      "& Chakravarty, 2022). The Chatbot industry has seen \n",
      "significant growth, especially in cloud-based services, \n",
      "driven by recent advancements.\n",
      "According to Gupta et al. (2020), ChatGPT, which \n",
      "stands for Chat Generative Pre-Trained Transformer, is a \n",
      "sophisticated Chatbot powered by AI. It was developed \n",
      "by OpenAI and officially released on 30th November, \n",
      "2022. ChatGPT generates original content in response to \n",
      "both simple and complex prompts (Panda & Kaur, 2023). \n",
      "ChatGPT allows users to shape and guide conversations \n",
      "according to their preferences regarding length, format, \n",
      "style, level of detail and language (Lock, 2022).\n",
      "Literature Review\n",
      "The provided literature review discusses various studies \n",
      "related to AI and its applications in different fields. \n",
      "Khosravi et al. (2023) conducted a study on “Chatbots \n",
      "and ChatGPT” and analysed the scientific literature \n",
      "on Chatbots and ChatGPT. The annual growth rate of \n",
      "literature indicates the tide of research is roughly 19–\n",
      "Global Research Trends on Chatbot and ChatGPT in the 21st Century      9\n",
      "27%  yearly application from COVID-19 and Ontology \n",
      "research turn to mental health and task analysis. Bawack \n",
      "et al. (2022) examine the uses of AI in E-Commerce \n",
      "through bibliometric methods. The finding indicates \n",
      "that China and the USA are leading countries using AI \n",
      "for E-commerce, and recommender systems are the most \n",
      "emerging technology. The study shows that optimisation, \n",
      "sentiment analysis and AI-related technologies are the \n",
      "main themes of research. Jimma Bahiru Legesse (2023) \n",
      "explores the uses of AI in healthcare. Scopus database \n",
      "was used for data download, and a drastic change has \n",
      "been noted after 2012 for research growth. A total \n",
      "of nine countries published 96.85% of publications, \n",
      "and the USA was the leading country with 41.84% of \n",
      "literature. The keyword analysis indicates that machine \n",
      "learning, electronic health records and natural language \n",
      "processing were the most frequently used keywords. \n",
      "The uses of AI not ed for COVID-19, diabetes, mental \n",
      "health, asthma, dementia and cancer treatment and data \n",
      "management. Xu, D. (2022) analysed the uses of AI for \n",
      "biotechnology and applied microbiology research. The \n",
      "study focused on quantitative, qualitative and modeling \n",
      "analyses of the literature. The result demonstrates that \n",
      "128 countries are associated with the research; the USA \n",
      "is the most productive country, and the Chinese Academy \n",
      "of Science is the leading research institution among 584 \n",
      "institutions. Ho and Wang (2020) examine the published \n",
      "literature on AI from Science Citation Index Expanded \n",
      "(SCI-EEPANED). Findings indicate that the USA leads in \n",
      "single authorship, international collaboration, and average \n",
      "citation. Chinese Academy of Science, Islamic Azad \n",
      "University and Massachusetts Institute of Technology \n",
      "(MIT) were the most productive institutions. The USA, \n",
      "Canada and Switzerland are the most collaborative \n",
      "countries in international collaboration. In cluster \n",
      "analysis, neural networks, learning and prediction are \n",
      "prolific keywords. Lucena et al. (2019) investigate AI in \n",
      "higher education. The finding indicates that research in \n",
      "AI has increased worldwide at a rapid pace. B. C. Biswas \n",
      "(2007) examined the research on Botany from 1994 to \n",
      "2003. The results demonstrate that multiple authorship \n",
      "dominates on single authorship. With 59%, citation books \n",
      "secure first positions, followed by articles 41%. The USA \n",
      "is the most productive country. The average number of \n",
      "38 citations per article is found. The average length of \n",
      "papers is 11.45 pages. Most of the articles have been \n",
      "published by academic institutions. The keywo rd analysis is categorised into 10 clusters, and the author generalised \n",
      "that the scientific studies associated with deep learning \n",
      "and machine learning.\n",
      "The literature review reveals the escalating significance \n",
      "of AI research on a global scale, permeating diverse \n",
      "domains. It underscores the USA’s consistent prominence \n",
      "in the realm of AI research, both in terms of leading \n",
      "countries and influential institutions. Moreover, it exposes \n",
      "the dynamic nature of research trends, illustrating shifts \n",
      "over time that mirror evolving societal priorities and the \n",
      "emergence of new technologies. A notable example is \n",
      "the transition from an emphasis on COVID-19 to mental \n",
      "health and task analysis within the Chatbot and ChatGPT \n",
      "research landscape, signifying a response to changing \n",
      "societal needs.\n",
      "Research Objectives\n",
      " ●To examine the Annual Growth Rate of Publication \n",
      "and average citation per year. \n",
      " ●To relocate the most relevant source. \n",
      " ●To analyse the most productive authors.\n",
      " ●To explore the most productive countries and current \n",
      "research trends.\n",
      "Methodology\n",
      "The bibliometric method was applied to investigate \n",
      "research trends on Chabot and ChatGPT. The bibliometric \n",
      "method is used for performance analysis, scientific \n",
      "mapping and developing subjects and prolific authors, \n",
      "institutions, nations, organisations and journals. To \n",
      "explore the research trend, an analysis of keywords, \n",
      "citation patterns and thematic analysis has been performed \n",
      "(Shollapur, 2023). The data were extracted from the \n",
      "Scopus database on 18th August 2023 using the keyword \n",
      "“Chatbot”  OR “ChatGPT” within the article title, abstract \n",
      "and keywords search criteria, and 6478 documents were \n",
      "retrieved and downloaded in CSV files, including all \n",
      "bibliographic information, number of citations, abstract, \n",
      "references, author and title keywords, funding details, \n",
      "etc. MS Excel R software (Biblioshiny) has been used \n",
      "for data analysis and visualisation. The information about \n",
      "the essential elements of data available on Chatbot and \n",
      "ChatGPT, like annual scientific Production rate, average \n",
      "citation, etc., is depicted in Table 1. The conference papers \n",
      "10      International Journal of Information Studies and Libraries Volume 8 Issue 2 July-December 2023\n",
      "(2969) dominate article (2195), which indicates that the \n",
      "research field is in the emergence stage at the global level. \n",
      "Multi-author documents dominate author publications, and citation per document was found to be significant \n",
      "(7.5). The primary information data shows considerable \n",
      "scope for future research in AI tools.\n",
      "Table 1:  Main Information about Dataset\n",
      "Description Results Description Results\n",
      "Timespan 2002:2023 DOCUMENT TYPES  \n",
      "Sources (Journals, Books, etc.) 2656 Article 2195\n",
      "Documents 6478 Book 10\n",
      "Annual Growth Rate % 43.89 Book chapter 167\n",
      "Document Average Age 1.96 Conference paper 2969\n",
      "Average citations per doc 7.5 Conference review 285\n",
      "References 163425 Data paper 2\n",
      "Keywords Plus (ID) 16124 Erratum 10\n",
      "Author’s Keywords (DE) 10719 Letter 280\n",
      "Authors 17517 Review 213\n",
      "Authors of single-authored docs 626 Short survey 17\n",
      "Single-authored docs 704  \n",
      "Co-Authors per Doc 3.67  \n",
      "International co-authorships % 16.84   \n",
      "Data Analysis and Discussion\n",
      "Annual Scientific Production and Average \n",
      "Citation Per Year\n",
      "Fig. 1 demonstrates the annual scientific growth of \n",
      "research, which was expanding slowly from 2002 to 2017, \n",
      "not more than 2% annually. After that, the exponential \n",
      "growth rate has been observed to be 32.1% annually. The \n",
      "last five-year growth rate for the year 2019 (613, 9.46%), \n",
      "2020 (804, 12.41%), 2021 (1091, 16.84%), 2022 (1271, \n",
      "19.62%) and the highest publication 1862 (32.15%) noted for the year 2023. The average citation per year for the \n",
      "journals is depicted in Fig. 2. The average citation per \n",
      "year ranges between 0.05 and 6.67, with the lowest 0.05 \n",
      "received for 2002 and the highest 6.67 for 2015, followed \n",
      "by 2017 (5.58) and 2016 (5.54). The highest 2083 citation \n",
      "was noted for 2023, followed by 2022 (1271) and 2022 \n",
      "(1091). The growth rate of publication and citation \n",
      "patterns indicated that massive acceptance of research is \n",
      "going on in the scientific community, and technological \n",
      "advancements have a substantial impact.Documents  6478  Book  10 \n",
      "Annual Growth Rate %  43.89  Book chapter  167 \n",
      "Document Average Age  1.96 Conference paper  2969  \n",
      "Average citations per doc  7.5 Conference review  285 \n",
      "References  163425  Data paper  2 \n",
      "Keywords Plus (ID)  16124  Erratum  10 \n",
      "Author's Keywords (DE)  10719  Letter  280 \n",
      "Authors  17517  Review  213 \n",
      "Authors of single -authored docs  626 Short survey  17 \n",
      "Single -authored docs  704    \n",
      "Co-Authors per Doc  3.67    \n",
      "International co -authorships %  16.84      \n",
      " \n",
      " \n",
      "<Level A> Data Analysis and Discussion  \n",
      "<Level B> Annual Scientific Production and Average Citation per Year  \n",
      "Figure 1 demonstrates the annual scientific growth of research, which was expanding slowly from \n",
      "2002 to 2017, not more than 2% annually. After that, the exponential growth rate has been observed \n",
      "to be 32.1% annually. The last five -year growth rate for the year 2019 (613, 9.46%), 2020 (804, \n",
      "12.41%) , 2021 (1091, 16.84%) , 2022 (1271, 19.62%) and the highest publication 1862 (32.15%) \n",
      "noted for the year 2023. The average citation per year for the journals is depicted in Figure 2. The \n",
      "average citation per year ranges between 0.05 and 6.67, with the lowest 0.05 received for 2002 and \n",
      "the highest 6.67 for 2015, followed by 2017 (5.58) and 2016 (5.54). The highest 2083 citation was \n",
      "noted for 2023, followed by 2022 (1271) and 2022 (1091). The growth rate of publication and citation \n",
      "patterns indicated that massive acceptance of research is going on in the scientific community, and \n",
      "technological advancements have a substantial impact . \n",
      " \n",
      " \n",
      "Fig. 1: Annual Scientific Growth of Literature  \n",
      " \n",
      " Fig. 1:  Annual Scientific Growth of Literature\n",
      "Global Research Trends on Chatbot and ChatGPT in the 21st Century      11\n",
      "Most Relevant Source\n",
      "Table 2 demonstrates the ten most relevant sources on \n",
      "Chatbots where lecture notes in Computer Science are \n",
      "noted as the top source with the highest 356 publications \n",
      "and 2228 citations. ACM International Conference \n",
      "Proceeding Series was the second most published source \n",
      "with 220 publications, and the third most common  \n",
      "Fig. 2: Average Citations Per Year  \n",
      "<Level B> Most Relevant Source  \n",
      "Table 2 demonstrates the ten most relevant sources on Chatbots where lecture notes in Computer \n",
      "Science are noted as the top source with the highest 356 publications and 2228 citations. ACM \n",
      "International Conference Proceeding Series was the second most published source with 220 \n",
      "publications, and the third most  common  source found CEUR Workshop Proceedings with 168 \n",
      "publications. The citation pattern indicates that the Conference on Human Factors in Computing \n",
      "Systems Proceedings has the second highest citation number and highest number of citations per \n",
      "paper, 22.4, more than any regular journals. The sources from computer science and engineering, \n",
      "information science and bioinformatics dominate the research in Chatbots and ChatGPT .  \n",
      "Table 2 : Top Ten Cited Source  \n",
      "Sr. \n",
      "No. Sources (Journals, Conferences Etc.)  Articles  Citation  CPP  \n",
      "1 Lecture Notes in Computer  356 2228  6.3 \n",
      "2 ACM International Conference Proceeding Series  210 1003  4.8 \n",
      "3 Ceur Workshop Proceedings  176 244 1.4 \n",
      "4 Lecture Notes in Networks and Systems  119 85 0.7 \n",
      "5 Communications In Computer and Information Science  112 320 2.9 \n",
      "6 Conference On Human Factors in Computing Systems – \n",
      "Proceedings  93 2080  22.4  \n",
      "7 Advances In Intelligent Systems and Computing  88 451 5.1 \n",
      "8 Journal Of Medical Internet Research  86 1674  19.5  \n",
      "9 Annals Of Biomedical Engineering  68 183 2.7 \n",
      "10 Lecture Notes in Electrical Engineering  49 67 1.4 \n",
      " \n",
      "The core journals of any research domain can be identified using Bradford’s law given by Samuel C. \n",
      "Bradford in 1934. The analysis divides all journals into three groups; each represents about one -third \n",
      "of all articles. The research (R software) has been depicted in Figure 3; a total of 2656 journals are \n",
      "identified in this study. First is a core zone, which has 47 (1.769%) journals with 2138 (33%) articles, \n",
      "Fig. 2:  Average Citations Per Year\n",
      "source found CEUR Workshop Proceedings with 168 \n",
      "publications. The citation pattern indicates that the \n",
      "Conference on Human Factors in Computing Systems \n",
      "Proceedings has the second highest citation number and \n",
      "highest number of citations per paper, 22.4, more than \n",
      "any regular journals. The sources from computer science \n",
      "and engineering, information science and bioinformatics \n",
      "dominate the research in Chatbots and ChatGPT. \n",
      "Table 2:  Top Ten Cited Source\n",
      "Sr. No. Sources (Journals, Conferences Etc.) Articles Citation CPP\n",
      "1 Lecture Notes in Computer 356 2228 6.3\n",
      "2 ACM International Conference Proceeding Series 210 1003 4.8\n",
      "3 Ceur Workshop Proceedings 176 244 1.4\n",
      "4 Lecture Notes in Networks and Systems 119 85 0.7\n",
      "5 Communications In Computer and Information Science 112 320 2.9\n",
      "6 Conference On Human Factors in Computing Systems – Proceedings 93 2080 22.4\n",
      "7 Advances In Intelligent Systems and Computing 88 451 5.1\n",
      "8 Journal Of Medical Internet Research 86 1674 19.5\n",
      "9 Annals Of Biomedical Engineering 68 183 2.7\n",
      "10 Lecture Notes in Electrical Engineering 49 67 1.4\n",
      "The core journals of any research domain can be identified \n",
      "using Bradford’s law given by Samuel C. Bradford in \n",
      "1934. The analysis divides all journals into three groups; \n",
      "each represents about one-third of all articles. The \n",
      "research (R software) has been depicted in Fig. 3; a total of 2656 journals are identified in this study. First is a core \n",
      "zone, which has 47 (1.769%) journals with 2138 (33%) \n",
      "articles, the second zone is 650 (24.47%) journals with \n",
      "2203 (34%) articles and the third zone is 1959 (73.75%) \n",
      "journals with 2137 (33%) articles. \n",
      "12      International Journal of Information Studies and Libraries Volume 8 Issue 2 July-December 2023\n",
      "the second zone is 650 (24.47%) journals with 2203  (34%) articles and the third zone is 1959 \n",
      "(73.75%) journals with 2137  (33%) articles.  \n",
      " \n",
      "Fig. 3: Core Research Journals Analysis through Bradford Law  \n",
      "<Level B> Authorship pattern  \n",
      "Table 3 lists the top 10 most productive author, with Zhang Y securing the first position with 28 \n",
      "articles, followed by LEE J, Li Y and Liu Y with 27 articles each, Denecke K with 26 articles, Kim J \n",
      "with 25 articles, Zhang J 23 articles, Følstad A, Li J, Singh S, 22 articles each. Citation result \n",
      "indicates that Giovannoni G, Hawkes C, Lechner -Scott J, Levy M and Yeh A obtained the highest \n",
      "number of 82 citations each. The authorship analysis suggests that Chinese authors dominate the top \n",
      "ten lists as the most relevant authors except for one author, Singh S from India. The author with the \n",
      "highest citation indicates that the citation pattern differs from the top 10 most appropriate authors. \n",
      "There is a mix of countries regarding the highest citations, and no dominance has been noticed of \n",
      "Chinese authors . \n",
      "Table 3 : Top Ten Highest Productive Authors  \n",
      "Sr. \n",
      "No. Most Relevant Authors  \n",
      "Authors  Articles  \n",
      "1 Zhang Y  28 \n",
      "2 Lee J  27 \n",
      "3 Li Y 27 \n",
      "4 Liu Y  27 \n",
      "5 Denecke K  26 \n",
      "6 Kim J  25 \n",
      "7 Zhang J  23 \n",
      "8 Følstad A  22 \n",
      "9 Li J 22 \n",
      "10 Singh S  22 \n",
      " No. of Articles , Zone \n",
      "1 (47 journals) , \n",
      "2138, 33%\n",
      "No. of Articles , Zone \n",
      "2 (650 journals) , \n",
      "2203 , 34%No. of Articles , Zone \n",
      "3 (1959 journals) , \n",
      "2137, 33%No. of Articles\n",
      "Zone 1 (47 journals)\n",
      "Zone 2 (650 journals)\n",
      "Zone 3 (1959 journals)\n",
      "Fig. 3: Core Research Journals Analysis through \n",
      "Bradford Law\n",
      "Authorship Pattern\n",
      "Table 3 lists the top 10 most productive author, with \n",
      "Zhang Y securing the first position with 28 articles, \n",
      "followed by LEE J, Li Y and Liu Y with 27 articles each, \n",
      "Denecke K with 26 articles, Kim J with 25 articles, Zhang \n",
      "J 23 articles, Følstad A, Li J, Singh S, 22 articles each. \n",
      "Citation result indicates that Giovannoni G, Hawkes C, \n",
      "Lechner-Scott J, Levy M and Yeh A obtained the highest \n",
      "number of 82 citations each. The authorship analysis \n",
      "suggests that Chinese authors dominate the top ten lists as \n",
      "the most relevant authors except for one author, Singh S \n",
      "from India. The author with the highest citation indicates \n",
      "that the citation pattern differs from the top 10 most \n",
      "appropriate authors. There is a mix of countries regarding \n",
      "the highest citations, and no dominance has been noticed \n",
      "of Chinese authors.Table 3:  Top Ten Highest Productive Authors\n",
      "Sr. No.Most Relevant Authors\n",
      "Authors Articles\n",
      "1 Zhang Y 28\n",
      "2 Lee J 27\n",
      "3 Li Y 27\n",
      "4 Liu Y 27\n",
      "5 Denecke K 26\n",
      "6 Kim J 25\n",
      "7 Zhang J 23\n",
      "8 Følstad A 22\n",
      "9 Li J 22\n",
      "10 Singh S 22\n",
      "Lotka’s Law of Scientific Productivity was given by A. \n",
      "J. Lotka in 1926. It describes the frequency of author \n",
      "publication in a field, which generalises how many articles \n",
      "an author publishes on a particular subject during a specific \n",
      "time frame. It is denoted by Xn Y= Constant, where Y \n",
      "is the frequency of authors making n contributions each \n",
      "(Bensman, S. J., & Smolinsky, L. J. 2017 ). The analysis \n",
      "is depicted in Fig. 4, where one article was written by \n",
      "14511 authors, two articles by 1822 authors, three articles \n",
      "by 564 authors, four articles by 247 authors, five articles \n",
      "by 128 authors, six articles by 66 authors, seven articles \n",
      "by 53 authors, eight articles by 33 author, nine articles by \n",
      "16 authors, 10 articles by 12 authors and 173 articles by \n",
      "more than 13 authors. It indicates that individual research \n",
      "dominates multi-author research patterns. Lotka’s Law of Scientific Productivity was given by A. J. Lotka in 1926. It describes the frequency of \n",
      "author publication in a field, which generalises how many articles an author publishes on a particular \n",
      "subject during a specific time frame. It is denoted by Xn Y= Constant, where Y is the frequency of \n",
      "authors making n contributions each ( Bensman, S. J., & Smolinsky, L. J. 2017 ). The analysis is \n",
      "depicted in Figure 4, where one article was written by 14511 authors, two articles by 1822 authors, \n",
      "three articles by 564 authors, four articles by 247 authors, five articles by 128 authors, six articles by \n",
      "66 authors, seven articles by 53 authors, eight articles by 33 author, nine articles by 16 authors, 10 \n",
      "articles by 12 authors  and 173 articles by more than 13 authors. It indicates that individual research \n",
      "dominates multi -author research patterns.  \n",
      "Fig. 4: Lotka’s Law of Scientific Productivity  \n",
      " \n",
      "<Level B> Most Relevant Affiliations  \n",
      "A total of 3554 institutions, universities and organisations were identified for publishing literature on \n",
      "the Chatbot and ChatGPT. Figure 7 demonstrates the top 10 affiliations based on their publications. \n",
      "The University of California holds the top position with 99 publications, followed by Bina Nusantara \n",
      "University with 51 publications; the National University of Singapore with 48 publications; the \n",
      "University of Hong Kong with 48 publications; Stanford University and the University of Toronto \n",
      "with 47 publi cations each, National Tsing Hua University, Ulster University and the University of \n",
      "Auckland with 43 publications each. The analysis indicates that universities dominate research \n",
      "institutions regarding publications, showing the academic use of A I. \n",
      "Fig. 4:  Lotka’s Law of Scientific Productivity\n",
      "Global Research Trends on Chatbot and ChatGPT in the 21st Century      13\n",
      "Most Relevant Affiliations\n",
      "A total of 3554 institutions, universities and organisations \n",
      "were identified for publishing literature on the Chatbot \n",
      "and ChatGPT. Fig. 5 demonstrates the top 10 affiliations \n",
      "based on their publications. The University of California \n",
      "holds the top position with 99 publications, followed \n",
      "by Bina Nusantara University with 51 publications; the National University of Singapore with 48 publications; \n",
      "the University of Hong Kong with 48 publications; \n",
      "Stanford University and the University of Toronto with \n",
      "47 publications each, National Tsing Hua University, \n",
      "Ulster University and the University of Auckland with 43 \n",
      "publications each. The analysis indicates that universities \n",
      "dominate research institutions regarding publications, \n",
      "showing the academic use of AI.\n",
      " \n",
      "Figure 5. Top 10 Most Contributing Organizations  \n",
      " \n",
      "<Level B> Citation and publication pattern of countries  \n",
      "Fig. 8 shows the country -wise citation and publication pattern. Analysis indicates the research \n",
      "contribution of 91 countries. USA is leading with 470 Single Country Publications (SCP) and 74 \n",
      "Multiple Country Publications (MCP), followed by India 366 (SCP) and 36 (MCP), China 288 (SCP) \n",
      "and 98 (MCP), UK 137 (SCP) and 55 (MCP), Germany 141 (SCP) and 26 (MCP), Italy 133 (SCP) \n",
      "and 31 (MCP), Korea 125 (SCP) and 23 (MCP), Australia 69 (SCP) and 32 (MCP)  and Spain 75 \n",
      "(SCP) and 23 (MCP).  \n",
      " \n",
      "Fig. 6: Top 15 Most Productive Countries  \n",
      "Table 5 indicates the citation pattern of the top 10 countries. USA received the highest citation 5145 \n",
      "with 9.50 Citation Per Paper (CPP), followed by China 2654 with 6.90 (CPP), United Kingdom 2555 \n",
      "with 13.30 (CPP), Korea 1555 with 10.5 (CPP), Germany 1425 with 8.50 (CPP), India 1308 with 3.30 \n",
      "(CPP), Australia 1290 with 12.80 (CPP), Italy 1137 with 6.90 (CPP), Norway 924 with 18.90 (CPP) \n",
      "and Spain 792 with 8.10 (CPP). The country citation pattern indicates that the USA dominates on the \n",
      "Fig. 5:  Top 10 Most Contributing Organizations\n",
      "Citation and Publication Pattern of Countries\n",
      "Fig. 6 shows the country-wise citation and publication \n",
      "pattern. Analysis indicates the research contribution of \n",
      "91 countries. USA is leading with 470 Single Country \n",
      "Publications (SCP) and 74 Multiple Country Publications (MCP), followed by India 366 (SCP) and 36 (MCP), \n",
      "China 288 (SCP) and 98 (MCP), UK 137 (SCP) and 55 \n",
      "(MCP), Germany 141 (SCP) and 26 (MCP), Italy 133 \n",
      "(SCP) and 31 (MCP), Korea 125 (SCP) and 23 (MCP), \n",
      "Australia 69 (SCP) and 32 (MCP) and Spain 75 (SCP) \n",
      "and 23 (MCP). \n",
      "Figure 5. Top 10 Most Contributing Organizations  \n",
      " \n",
      "<Level B> Citation and publication pattern of countries  \n",
      "Fig. 8 shows the country -wise citation and publication pattern. Analysis indicates the research \n",
      "contribution of 91 countries. USA is leading with 470 Single Country Publications (SCP) and 74 \n",
      "Multiple Country Publications (MCP), followed by India 366 (SCP) and 36 (MCP), China 288 (SCP) \n",
      "and 98 (MCP), UK 137 (SCP) and 55 (MCP), Germany 141 (SCP) and 26 (MCP), Italy 133 (SCP) \n",
      "and 31 (MCP), Korea 125 (SCP) and 23 (MCP), Australia 69 (SCP) and 32 (MCP)  and Spain 75 \n",
      "(SCP) and 23 (MCP).  \n",
      " \n",
      "Fig. 6: Top 15 Most Productive Countries  \n",
      "Table 5 indicates the citation pattern of the top 10 countries. USA received the highest citation 5145 \n",
      "with 9.50 Citation Per Paper (CPP), followed by China 2654 with 6.90 (CPP), United Kingdom 2555 \n",
      "with 13.30 (CPP), Korea 1555 with 10.5 (CPP), Germany 1425 with 8.50 (CPP), India 1308 with 3.30 \n",
      "(CPP), Australia 1290 with 12.80 (CPP), Italy 1137 with 6.90 (CPP), Norway 924 with 18.90 (CPP) \n",
      "and Spain 792 with 8.10 (CPP). The country citation pattern indicates that the USA dominates on the \n",
      "Fig. 6:  Top 15 Most Productive Countries\n",
      "14      International Journal of Information Studies and Libraries Volume 8 Issue 2 July-December 2023\n",
      "Table 4 indicates the citation pattern of the top 10 \n",
      "countries. USA received the highest citation 5145 with \n",
      "9.50 Citation Per Paper (CPP), followed by China 2654 \n",
      "with 6.90 (CPP), United Kingdom 2555 with 13.30 (CPP), \n",
      "Korea 1555 with 10.5 (CPP), Germany 1425 with 8.50 \n",
      "(CPP), India 1308 with 3.30 (CPP), Australia 1290 with 12.80 (CPP), Italy 1137 with 6.90 (CPP), Norway 924 \n",
      "with 18.90 (CPP) and Spain 792 with 8.10 (CPP). The \n",
      "country citation pattern indicates that the USA dominates \n",
      "on the total number of papers and citations, where the \n",
      "highest citation Per Paper noted for Norway is 16.07, \n",
      "which suggests the research quality.\n",
      "Table 4:  Top 10 Most Cited and Publishing Country\n",
      "Sr. No. Country Total Publication Total Citation CPP\n",
      "1 USA 1314 15330 11.6\n",
      "2 India 934 4642 4.95\n",
      "3 United Kingdom 477 5312 11.01\n",
      "4 China 474 4216 8.8\n",
      "5 Germany 382 2761 7.2\n",
      "6 Italy 308 2594 8.8\n",
      "7 South Korea 235 2592 11\n",
      "8 Australia 216 2592 11.09\n",
      "9 Taiwan 199 1507 7.6\n",
      "10 Norway 82 1355 16.07\n",
      "Trend Topic and Keyword Analysis\n",
      "The trend topics have been analysed to locate the 20 \n",
      "most emerging issues between the years 2012 and 2023. \n",
      "Fig. 7 indicates that for the year 2023, the keywords \n",
      "ChatGPT occurred (483) times, followed by AI (153) \n",
      "and education (99). Furthermore, in the year 2022, AI \n",
      "(986), machine learning (357) and COVID-19 (151). \n",
      "In the year 2021, the terms Chatbot (2260), Chatbots \n",
      "(605) and natural language processing (502) are the \n",
      "most frequently used terms. Year 2020, noted as question \n",
      "answering (37), dialog system (36) and human-computer \n",
      "interaction (34). In the year 2019, intelligent agents (13), \n",
      "conversation (13) and agent (11). For the year 2018, aim \n",
      "(53), pattern matching (12) and question answering (7). \n",
      "The results of the keyword indicate that human, humans \n",
      "and software most relevant and highest time occurred in 2023, followed by Chatbots, AI and natural language \n",
      "processing in 2022, natural language processing systems, \n",
      "conversational agents and students in 2021, human-\n",
      "computer interaction, user interfaces, semantics in 2020, \n",
      "Chabot, human engineering, intelligent agent in 2019, \n",
      "ubiquitous computing, AI markup language, Turing test \n",
      "in 2018, aim in 2017 and latent semantic analysis in \n",
      "2016, java programming language, human-computer \n",
      "dialogues, e-learning environment in 2014 and virtual \n",
      "worlds, non-player character in 2010 and mathematical \n",
      "models in 2005. This shows that Chatbot and Chatbot \n",
      "have various research subdomains identified from the \n",
      "beginning of research years, especially the first decade of \n",
      "the 21st century. The second decade is heavily involved \n",
      "in research of semantics, Markup languages, high-\n",
      "performance programming, and task-based systems, later \n",
      "converted into AI and further as AI assistance systems.\n",
      "Global Research Trends on Chatbot and ChatGPT in the 21st Century      15\n",
      " \n",
      " \n",
      "Fig. 7: Top 20 year wise keywords with highest trends  \n",
      " \n",
      " \n",
      "<Level B> Co-occurrence of Keywords and Thematic Analysis  \n",
      "Figure 8 maps the occurrence of keywords and the three clusters identified. Cluster 1 main theme was \n",
      "AI, with sub -themes machine learning, mental health, health care, Chatbot , social media, COVID -19, \n",
      "decision making, etc. The cluster 2 main themes found Humans, including sub -theme humans, article, \n",
      "adults, software, language, communication, interpersonal communication, letter, control study, etc. \n",
      "The main themes Chatbots noted for cluster 3, with sub -themes Chatbot , natural language, \n",
      "conversational agents, d eep learning, learning system, human -computer interaction, user interface, \n",
      "language processing, learning algorithms, e -learning, semantics, computational linguistics, knowledge \n",
      "base system, machine learning, behavio ural research, social networking (online), speech processing, \n",
      "virtual assistance, dialogue system, sentiment analysis, information use, language model and user \n",
      "experience. The thematic research of co -occurrence keywords indicates that the main themes are not \n",
      "scattered in different areas and cover  the utilisation of AI with sophisticated systems for enhancing \n",
      "excellence to support  human centric activity.  \n",
      "Fig. 7:  Top 20 Year Wise Keywords with Highest Trends\n",
      "Co-Occurrence of Keywords and Thematic \n",
      "Analysis\n",
      "Fig. 8 maps the occurrence of keywords and the three \n",
      "clusters identified. Cluster 1 main theme was AI, with \n",
      "sub-themes machine learning, mental health, health care, \n",
      "Chatbot, social media, COVID-19, decision making, etc. \n",
      "The cluster 2 main themes found Humans, including \n",
      "sub-theme humans, article, adults, software, language, \n",
      "communication, interpersonal communication, letter, \n",
      "control study, etc. The main themes Chatbots noted for \n",
      "cluster 3, with sub-themes Chatbot, natural language, conversational agents, deep learning, learning system, \n",
      "human-computer interaction, user interface, language \n",
      "processing, learning algorithms, e-learning, semantics, \n",
      "computational linguistics, knowledge base system, \n",
      "machine learning, behavioural research, social networking \n",
      "(online), speech processing, virtual assistance, dialogue \n",
      "system, sentiment analysis, information use, language \n",
      "model and user experience. The thematic research of co-\n",
      "occurrence keywords indicates that the main themes are \n",
      "not scattered in different areas and cover the utilisation of \n",
      "AI with sophisticated systems for enhancing excellence to \n",
      "support human centric activity.\n",
      " \n",
      "Fig. 8: Co-Occurrence of Keyw ords  \n",
      " \n",
      "Figure 9 indicates the emerging theme for the present study. Two themes have been analysed : the \n",
      "primary and niche themes of Chatbot and ChatGPT. The basic theme represented by Chatbots is \n",
      "found as the central theme with (1809) documents, followed by chatbot (891), natural language \n",
      "processing system (740), natural language processing (462), conversational agent (452), natural \n",
      "language (411), students (326), learning system (311), deep learning (301) and human -computer \n",
      "interaction (271) documents. In niche themes, AI with 1022 documents found most relevant, followed \n",
      "by human (768), human (525), article (307), adult (200), female (188), health care (179), mental \n",
      "health (176), male (167) and ChatGPT (143). The thematic analysis indicates that AI is mainly used to \n",
      "support human intelligence, not as independent research.  \n",
      " \n",
      "Fig. 9: Keywords Thematic Map   \n",
      "Cluster 1  Cluster 2  \n",
      " \n",
      "Cluster 3  \n",
      " \n",
      "Fig. 8:  Co-Occurrence of Keywords\n",
      "16      International Journal of Information Studies and Libraries Volume 8 Issue 2 July-December 2023\n",
      "Fig. 9 indicates the emerging theme for the present \n",
      "study. Two themes have been analysed: the primary \n",
      "and niche themes of Chatbot and ChatGPT. The basic \n",
      "theme represented by Chatbots is found as the central \n",
      "theme with (1809) documents, followed by chatbot \n",
      "(891), natural language processing system (740), \n",
      "natural language processing (462), conversational agent \n",
      "(452), natural language (411), students (326), learning system (311), deep learning (301) and human-computer \n",
      "interaction (271) documents. In niche themes, AI with \n",
      "1022 documents found most relevant, followed by human \n",
      "(768), human (525), article (307), adult (200), female \n",
      "(188), health care (179), mental health (176), male (167) \n",
      "and ChatGPT (143). The thematic analysis indicates that \n",
      "AI is mainly used to support human intelligence, not as \n",
      "independent research. \n",
      "Fig. 8: Co-Occurrence of Keyw ords  \n",
      " \n",
      "Figure 9 indicates the emerging theme for the present study. Two themes have been analysed : the \n",
      "primary and niche themes of Chatbot and ChatGPT. The basic theme represented by Chatbots is \n",
      "found as the central theme with (1809) documents, followed by chatbot (891), natural language \n",
      "processing system (740), natural language processing (462), conversational agent (452), natural \n",
      "language (411), students (326), learning system (311), deep learning (301) and human -computer \n",
      "interaction (271) documents. In niche themes, AI with 1022 documents found most relevant, followed \n",
      "by human (768), human (525), article (307), adult (200), female (188), health care (179), mental \n",
      "health (176), male (167) and ChatGPT (143). The thematic analysis indicates that AI is mainly used to \n",
      "support human intelligence, not as independent research.  \n",
      " \n",
      "Fig. 9: Keywords Thematic Map   \n",
      "Cluster 1  Cluster 2  \n",
      " \n",
      "Cluster 3  \n",
      " \n",
      "Fig. 9:  Keywords Thematic Map \n",
      "Result and Conclusion\n",
      "The study, encompassing an analysis of 6252 articles on \n",
      "Chatbot and ChatGPT from 2002 to 2023 retrieved from \n",
      "the Scopus database, reveals several noteworthy trends. \n",
      "The annual growth rate of ChatGPT literature stood at \n",
      "43.12%, exhibiting a prolonged period of growth between \n",
      "2002 and 2017, followed by a significant exponential \n",
      "surge post-2017. Particularly striking is the substantial \n",
      "increase in publication rates from 2019 to 2023, signifying \n",
      "a heightened interest and research activity within this \n",
      "field. While the average yearly citation rate displayed \n",
      "variations, the year 2023 garnered the highest number \n",
      "of citations, with 1862 citations, closely trailed by 2022 \n",
      "and 2021. In terms of sources, lecture notes in Computer \n",
      "Science (including Subseries Lecture Notes in AI and \n",
      "Bioinformatics) emerged as the most relevant source, \n",
      "with 356 articles. Notably, Weizenbaum J claimed the \n",
      "spotlight as the most pertinent author with 621 citations. \n",
      "On a local impact scale, the Conference on Human \n",
      "Factors in Computing Systems – Proceedings held the \n",
      "top rank with a 25 H-index. Author Zhang Y proved to be the most prolific, boasting 28 publications, followed \n",
      "closely by Lee J and Li Y , each with 27 publications. \n",
      "Regarding impact, author Folstad A claimed the foremost \n",
      "position with an 11 h-index, trailed by Kim S, Kowatsch \n",
      "T and Lombardi, each with a 10 h-index. Notably, the \n",
      "University of California and Bina Nusantara University \n",
      "secured the leading positions as the top two contributing \n",
      "institutions. On a country level, the USA maintained its \n",
      "top position with 544 publications, closely followed by \n",
      "India with 402 publications. However, a shift occurred \n",
      "concerning citations received, with the USA leading in \n",
      "citation reception with 5145 citations, averaging 9.50 \n",
      "citations per article, followed by China with 2654 citations \n",
      "and an average of 6.90 citations per article. While India \n",
      "held the second position in terms of publications, its \n",
      "ranking dropped to third in citation impact, highlighting \n",
      "the importance of literature quality in attracting research \n",
      "attention. Remarkably, Norway achieved the highest \n",
      "average citations per article, with 28.90, followed by \n",
      "Norway once more with 18.90 citations per article. Lastly, \n",
      "the study’s author keyword analysis spotlighted Chatbot \n",
      "occurring 2303 times, followed by AI (104 times), \n",
      "Global Research Trends on Chatbot and ChatGPT in the 21st Century      17\n",
      "Chatbots (616 times), ChatGPT (563 times) and natural \n",
      "language processing (511 times). In the contemporary \n",
      "landscape, ChatGPT, AI and education emerged as the \n",
      "most trending topics in 2023, while AI machine learning \n",
      "and COVID-19 took center stage in 2022. The author \n",
      "keyword analysis unveiled humans and software as the \n",
      "most trending topics in 2023, whereas chatbots, AI and \n",
      "natural language processing dominated the discourse in \n",
      "2022. Co-occurrence analysis identified three primary \n",
      "clusters: Artificial intelligence, encompassing machine \n",
      "learning, mental health, and ChatGPT-related documents; \n",
      "Chatbots, covering Chatbots, natural language or deep \n",
      "learning documents; and Humans, delving into software \n",
      "and language-related subjects. In summation, this study \n",
      "effectively fulfilled its objectives and provides valuable \n",
      "insights into the global research trends in Chatbots and \n",
      "ChatGPT, serving as a valuable resource for the research \n",
      "community and policymakers.\n",
      "References\n",
      "Jimma, B. L. (2023). Artificial intelligence in healthcare: \n",
      "A bibliometric analysis. Telematics and Informatics \n",
      "Reports , 100041. Retrieved July 2, 2023.\n",
      "Agrawal, A., Gans, J., & Goldfarb, A. (2022). ChatGPT \n",
      "and how AI disrupts industries. Harvard Business \n",
      "Review . Retrieved July 3, 2023, from https://hbr.\n",
      "org/2022/12/chatgpt-andhow-ai-disrupts-industries\n",
      "Altaf, Y . (2023). 5 ways ChatGPT will impact digital \n",
      "marketing. Entrepreneur . Retrieved July 4, 2023, \n",
      "from https://www.entrepreneur.com/growing-\n",
      "a-business/5-ways-chatgpt-will-impact-digital-\n",
      "marketing/446208\n",
      "Bishop, C. M. (1994). Neural networks and their \n",
      "applications. Review of Scientific Instruments , 65, \n",
      "article 1803. doi:https://doi.org/10.1063/1.1144830\n",
      "Biswas, S. S. (2023). Potential use of ChatGPT in global \n",
      "warming. Annals of Biomedical Engineering , \n",
      "51, 1126-1127. doi:https://doi.org/10.1007/\n",
      "s10439-023-03171-8\n",
      "Brockman, G., Cheung, V ., Pettersson, L., Schneider, J., \n",
      "Schulman, J., Tang, J., & Zaremba, W. (2016). Openai \n",
      "gym. arXiv . Retrieved July 8, 2023; doi:https//doi.\n",
      "org/10.48550/arXiv.1606.01540\n",
      "Budzianowski, P., & Vulić, I. (2019). Hello, it’s GPT-2-\n",
      "-how can I help you? towards the use of pretrained \n",
      "language models for task-oriented dialogue systems. arXiv . Retrieved July 8, 2023; doi:https://doi.\n",
      "org/10.48550/arXiv.1907.05774\n",
      "Cherian, A., Peng, K. C., Lohit, S., Smith, K., & Tenenbaum, \n",
      "J. B. (2022). Are Deep Neural Networks SMARTer \n",
      "than Second Graders? arXiv. Retrieved July 9, 2023; \n",
      "doi:https://doi.org/10.48550/arXiv.2212.09993\n",
      "Dale, R. (2017). NLP in a post-truth world. Natural \n",
      "Language Engineering , 23(2), 319-324.\n",
      "Dale, R. (2021). GPT-3 What’s it good for? Natural \n",
      "Language Engineering , 27(1), 113-118.\n",
      "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). \n",
      "Bert: Pre-training of deep bidirectional transformers \n",
      "for language understanding. arXiv . Retrieved \n",
      "July 25, 2023; doi:https://doi.org/10.48550/\n",
      "arXiv.1810.04805\n",
      "Diaz, M. (2023). How to use ChatGPT: Everything you \n",
      "need to know. ZDNET. Retrieved July 27, 2023. \n",
      "Else, H. (2023). Abstracts written by ChatGPT fool \n",
      "scientists. Nature,  613(7344), 423 doi:https://doi.\n",
      "org/10.1038/d41586-023- 00056-7\n",
      "Erhan, D., Bengio, Y ., Courville, A., Manzagol, P., & \n",
      "Vincent, P. (2010). Why does unsupervised pre-\n",
      "training help deep learning. Journal of Machine \n",
      "Learning Research , 11, 625- 660.\n",
      "Floridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, \n",
      "scope, limits, and consequences. Minds and \n",
      "Machines , 30(4), 681-694.\n",
      "Goh, G., Cammarata, N., V oss, C., Carter, S., Petrov, \n",
      "M., Schubert, L., Radford, A., & Olah, C. \n",
      "(2021). Multimodal neurons in artificial neural \n",
      "networks . Retrieved July 31, 2023; doi:https://doi.\n",
      "org/10.23915/distill.00030\n",
      "King, M. R. (2022). The future of AI in medicine: A \n",
      "perspective from a chatbot.  Annals of Biomedical \n",
      "Engineering. Retrieved August 1, 2023; doi:https://\n",
      "doi.org/10.1007/s10439-022-03121-w\n",
      "Kirmani, A. R. (2022). Artificial intelligence-enabled \n",
      "science poetry. ACS Energy Letters, 8, 574-576. \n",
      "Retrieved August 1, 2023.\n",
      "Lee, C., Panda, P., Srinivasan, G., & Roy, K. (2018). \n",
      "Training deep spiking convolutional neural networks \n",
      "with STDP-based unsupervised pre-training \n",
      "followed by supervised fine-tuning. Frontiers in \n",
      "Neuroscience, 12,  article 435. Retrieved August 7, \n",
      "2023.\n",
      "Liu, X., Zheng, Y ., Du, Z., Ding, M., Qian, Y ., Yang, \n",
      "Z., & Tang, J. (2021). GPT understands, too. \n",
      "arXiv. Retrieved August 1, 2023; doi:https://doi.\n",
      "org/10.48550/arXiv.2103.10385 \n",
      "18      International Journal of Information Studies and Libraries Volume 8 Issue 2 July-December 2023\n",
      "Lucy, L., & Bamman, D. (2021). Gender and representation \n",
      "bias in GPT-3 generated stories. Proceedings of the \n",
      "Workshop on Narrative Understanding , 3, 48-55.\n",
      "Lund, B. D., & Wang, T. (2023). Chatting about ChatGPT: \n",
      "How may AI and GPT impact academia and libraries? \n",
      "Library Hi Tech News , 40(3), 26-29. doi:https://doi.\n",
      "org/10.1108/lhtn-01-2023-0009\n",
      "Mandelaro, J. (2023, February 27). How will AI chatbots \n",
      "like ChatGPT affect higher education? News Center. \n",
      "Retrieved August 7, 2023.\n",
      "Mok, A., & Zinkula, J. (2023, February 2). ChatGPT \n",
      "may be coming for our jobs. Here are the \n",
      "10 roles that AI is most likely to replace. \n",
      "Business Insider Africa . Retrieved August 7, \n",
      "2023, from https://africa.businessinsider.com/\n",
      "news/chatgpt-may-becoming-for-our-jobs-\n",
      "here-are-the-10-roles-that-ai-ismost-likely-to/\n",
      "grmgtk3\n",
      "Q.ai - Powering a Personal Wealth Movement. (2023). \n",
      "What is ChatGPT? How AI is transforming multiple \n",
      "industries. Forbes . Retrieved August 8, 2023, from \n",
      "https://www.forbes.com/sites/qai/2023/02/01/what-\n",
      "ischatgpt-how-ai-is-transforming-multipleindustries\n",
      "/?sh=64e915ce728e\n",
      "Panda, S., & Chakravarty, R. (2022). Adapting intelligent \n",
      "information services in libraries: A case of smart \n",
      "AI chatbots. Library Hi Tech News , 39(1), 12-15. \n",
      "doi:https://doi.org/10.1108/lhtn-11-2021-0081Panda, S., & Kaur, N. (2023). Exploring the viability of \n",
      "ChatGPT as an alternative to traditional ChatBot \n",
      "systems in library and information centers. Library \n",
      "Hi Tech News , 40(3), 22-25. doi:http://dx.doi.\n",
      "org/10.1108/lhtn-02-2023-0032\n",
      "Sachdev, S. (2023). ChatGPT and its impact \n",
      "on society. The Times of India . Retrieved \n",
      "August 12, 2023, from https://timesofindia.\n",
      "indiatimes.com/readersblog/marketing-savvy/\n",
      "chatgpt-and-its-impact-on-society-50445  \n",
      "UNESCO. (2021). AI and education: Guidance for policy-\n",
      "makers. Retrieved August 15, 2023 from https://\n",
      "unesdoc.unesco.org/ark:/48223/ pf0000376709\n",
      "UNESCO. (2021). Recommendation on the ethics of \n",
      "artificial intelligence . Retrieved August 20, 2023, \n",
      "from https://unesdoc.unesco.org/ark:/48223/\n",
      "pf0000381137\n",
      "UNESCO World Commission on the Ethics of Scientific \n",
      "Knowledge and Technology. (2019). Preliminary \n",
      "study on the ethics of artificial intelligence.  Retrieved \n",
      "August 20, 2023, from https:// unesdoc.unesco.org/\n",
      "ark:/48223/pf0000367823\n",
      "Bensman, S. J., & Smolinsky, L. J. (2017). Lotka’s \n",
      "inverse square law of scientific productivity: Its \n",
      "methods and statistics. Journal of the Association for \n",
      "Information Science and Technology, 68 (7), 1786-\n",
      "1791.  Retrieved August 25, 2023.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "text=\"\"\n",
    "pdf_docs = [r'C:/vidit/codes/PROJECTS/Tutor-app/artifacts/ssrn-4878177.pdf']\n",
    "for pdf in pdf_docs:\n",
    "    pdf_reader= PdfReader(pdf)\n",
    "    for page in pdf_reader.pages:\n",
    "        text+= page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English\\n\\nHindi\\n\\nGujarati\\n\\nSpecials\\n\\nMoneycontrol Trending Stock\\n\\nInfosys\\xa0INE009A01021, INFY, 500209\\n\\nState Bank of India\\xa0INE062A01020, SBIN, 500112\\n\\nYes Bank\\xa0INE528G01027, YESBANK, 532648\\n\\nBank Nifty\\n\\nNifty 500\\n\\nQuotes\\n\\nMutual Funds\\n\\nCommodities\\n\\nFutures & Options\\n\\nCurrency\\n\\nNews\\n\\nCryptocurrency\\n\\nForum\\n\\nNotices\\n\\nVideos\\n\\nGlossary\\n\\nAll\\n\\nHello, Login Hello, LoginLog-inor Sign-UpMy AccountMy Profile My PortfolioMy WatchlistFREE Credit Score₹100 Cash RewardMy AlertsMy MessagesPrice AlertsMy Profile My PROMy PortfolioMy WatchlistFREE Credit Score₹100 Cash RewardMy AlertsMy MessagesPrice AlertsLogoutChat with UsDownload AppFollow us on:\\n\\nGo Ad-Free\\n\\nMy Alerts\\n\\nBudget 2        24MarketsHOMEINDIAN INDICESSTOCK ACTIONAll StatsTop GainersTop LosersOnly BuyersOnly Sellers52 Week High52 Week LowPrice ShockersVolume ShockersMost Active StocksGLOBAL MARKETSUS MARKETSSEASONALITY ANALYSISSTOCK SCANNERECONOMIC INDICATORSECONOMIC CALENDARMARKET ACTIONDashboardF&OFII & DII ActivityCorporate ActionEARNINGSCOMMODITYPRE MARKETRESEARCHAdviceBroker ResearchTechnicalsCURRENCYCRYPTOCURRENCYBIG SHARK PORTFOLIOSTECHNICAL TRENDSIPOBONDSWEBINARINTERVIEW SERIESOTHERSCryptocurrency NewsToolsNewsLATEST NEWSBudget 2024BUSINESSEconomyCompaniesMutual FundsPersonal FinanceIPOStartupReal EstateNATION & WORLDIndiaWorldPoliticsMARKETSStocksTechnical AnalysisEquity ResearchCommoditiesCurrencyGold RateSilver RateAQISPECIALTrendsLatest NewsOpinionExplainersMC BuzzMC FeaturesMC LearnTECHNOLOGYPersonal TechAutoFintechMEDIAPodcastPhotosVideosWeb StoriesCRYPTOCURRENCYOTHERSEntertainmentSportsLifestyleHealth and FitnessEducationJobsScienceAstroTravelTechPortfolioWatchlistCommoditiesMutual FundsEXPLOREHomeFind FundTop Ranked FundsPerformance TrackerSIP Performance TrackerETF PerformanceNFOTop Performing CategoriesLearnTOOLSReturns CalculatorLumpsum SIP BalancerDelay Cost CalculatorSIP ReturnMF FORUMTRACKYour MF InvestmentMF PricesMC 30Personal FinanceEXPLOREHomeInvestingInsuranceBankingFinancial PlanningPropertyToolsVideoAsk ExpertExplainerIncome Tax Filing GuideNPSFIXED DEPOSITFixed Deposit ComparisonFixed Deposit Interest CalculatorCorporate DepositsLOANS & CREDIT CARDSHomeLoans\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\xa0Car Loan CalculatorHome Loan CalculatorEducation Loan CalculatorCredit Card Debit Payoff CalculatorTAXIncome tax Filing GuideIncome Tax CalculatorEmergency Fund CalculatorTOOLSProvident Fund CalculatorAssets Allocation PlanningDebt Reduction PlannerDebt Evaluation CalculatorCurrent Expense CalculatorFREE CREDIT SCORE₹100 Cash RewardREAL ESTATE : TRENDS & ANALYTICSMC 30MONEYCONTROL - SECURENOW HEALTH INSURANCE RATINGSGOLD PRICE TODAYUpcoming Chat | Previous TranscriptsAll Schedule | Previous TranscriptForumEXPLORE FORUMHomepageMembership RulesForum TopicsAsk the ExpertTop BoardersUSER PROFILEFORUM TOPICSLatest ThreadsStocksIndexGoldPersonal FinanceJust PostedMF FORUMPOLLSLatest PollsHistorical PollsMarket SentimentsSUPPORTFAQsCode of ConductFeedbackWrite to usVideosVIDEOSHomepageVideos on DemandMarkets with Santo & CJMorning TradeCommodities chat with Manisha GuptaLet`s Talk JobsThe TenantDrive ReportBajar GupshupBits To BillionsPODCASTHomepagePodcast on DemandThe Week on Dalal StreetMarket MinutesMC Special PodcastSimply SavePolicy TalksLIVE TVHindiGujaratiInvest NowExpert Trading GuidancePowered By Trading Advisories, Courses & Webinars by Top Verified Experts.Unlock Your Trading Potential: Trade like Experts with SEBI registered creators, Learn from Courses & Webinars by India\\'s Finest Finance Experts.Invest NowPRO BusinessMarketsStocksEconomyCompaniesTrendsIPOOpinionEV Special\\n\\n>->MC_ENG_DESKTOP/MC_ENG_NEWS/MC_ENG_MARKETS_AS/MC_ENG_ROS_NWS_MKTS_AS_ATF_728\\n\\nGo PRO @₹99\\n\\nPRO\\n\\nAdvertisement\\n\\nRemove Ad\\n\\n\\n\\nBudget 2        24\\n\\nMarketsHOMEINDIAN INDICESSTOCK ACTIONAll StatsTop GainersTop LosersOnly BuyersOnly Sellers52 Week High52 Week LowPrice ShockersVolume ShockersMost Active StocksGLOBAL MARKETSUS MARKETSSEASONALITY ANALYSISSTOCK SCANNERECONOMIC INDICATORSECONOMIC CALENDARMARKET ACTIONDashboardF&OFII & DII ActivityCorporate ActionEARNINGSCOMMODITYPRE MARKETRESEARCHAdviceBroker ResearchTechnicalsCURRENCYCRYPTOCURRENCYBIG SHARK PORTFOLIOSTECHNICAL TRENDSIPOBONDSWEBINARINTERVIEW SERIESOTHERSCryptocurrency NewsTools\\n\\nNewsLATEST NEWSBudget 2024BUSINESSEconomyCompaniesMutual FundsPersonal FinanceIPOStartupReal EstateNATION & WORLDIndiaWorldPoliticsMARKETSStocksTechnical AnalysisEquity ResearchCommoditiesCurrencyGold RateSilver RateAQISPECIALTrendsLatest NewsOpinionExplainersMC BuzzMC FeaturesMC LearnTECHNOLOGYPersonal TechAutoFintechMEDIAPodcastPhotosVideosWeb StoriesCRYPTOCURRENCYOTHERSEntertainmentSportsLifestyleHealth and FitnessEducationJobsScienceAstroTravel\\n\\nTech\\n\\nPortfolio\\n\\nWatchlist\\n\\nCommodities\\n\\nMutual FundsEXPLOREHomeFind FundTop Ranked FundsPerformance TrackerSIP Performance TrackerETF PerformanceNFOTop Performing CategoriesLearnTOOLSReturns CalculatorLumpsum SIP BalancerDelay Cost CalculatorSIP ReturnMF FORUMTRACKYour MF InvestmentMF PricesMC 30\\n\\nPersonal FinanceEXPLOREHomeInvestingInsuranceBankingFinancial PlanningPropertyToolsVideoAsk ExpertExplainerIncome Tax Filing GuideNPSFIXED DEPOSITFixed Deposit ComparisonFixed Deposit Interest CalculatorCorporate DepositsLOANS & CREDIT CARDSHomeLoans\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\xa0Car Loan CalculatorHome Loan CalculatorEducation Loan CalculatorCredit Card Debit Payoff CalculatorTAXIncome tax Filing GuideIncome Tax CalculatorEmergency Fund CalculatorTOOLSProvident Fund CalculatorAssets Allocation PlanningDebt Reduction PlannerDebt Evaluation CalculatorCurrent Expense CalculatorFREE CREDIT SCORE₹100 Cash RewardREAL ESTATE : TRENDS & ANALYTICSMC 30MONEYCONTROL - SECURENOW HEALTH INSURANCE RATINGSGOLD PRICE TODAYUpcoming Chat | Previous TranscriptsAll Schedule | Previous Transcript\\n\\nForumEXPLORE FORUMHomepageMembership RulesForum TopicsAsk the ExpertTop BoardersUSER PROFILEFORUM TOPICSLatest ThreadsStocksIndexGoldPersonal FinanceJust PostedMF FORUMPOLLSLatest PollsHistorical PollsMarket SentimentsSUPPORTFAQsCode of ConductFeedbackWrite to us\\n\\nVideosVIDEOSHomepageVideos on DemandMarkets with Santo & CJMorning TradeCommodities chat with Manisha GuptaLet`s Talk JobsThe TenantDrive ReportBajar GupshupBits To BillionsPODCASTHomepagePodcast on DemandThe Week on Dalal StreetMarket MinutesMC Special PodcastSimply SavePolicy TalksLIVE TVHindiGujarati\\n\\nInvest NowExpert Trading GuidancePowered By Trading Advisories, Courses & Webinars by Top Verified Experts.Unlock Your Trading Potential: Trade like Experts with SEBI registered creators, Learn from Courses & Webinars by India\\'s Finest Finance Experts.Invest Now\\n\\nPRO\\n\\nBusiness\\n\\nMarkets\\n\\nStocks\\n\\nEconomy\\n\\nCompanies\\n\\nTrends\\n\\nIPO\\n\\nOpinion\\n\\nEV Special\\n\\nHome\\n\\nNews\\n\\nBusiness\\n\\nMarkets\\n\\nTrending Topics\\n\\nSensex Today\\n\\nOil Prices\\n\\nHDFC Bank\\n\\nKalyan Jewellers share price\\n\\nNephro Care Share Price\\n\\nMarket corrects post RBI MPC outcome| Bet on these top 10 rate-sensitive stocks\\n\\nExperts are not worried due to this market fall, in fact, it gives an opportunity to pick quality stocks including rate sensitive stocks on dips.\\n\\nSunil Shankar Matkar\\n\\nAugust 10, 2023 / 01:45 PM IST\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRBI holds repo rate at 6.5 percent, raises full year inflation forecast\\n\\nWatchlist\\n\\nPortfolio\\n\\nMessage\\n\\nSet Alert\\n\\nlive\\n\\nbselive\\n\\nnselive\\n\\nVolume \\n\\nTodays L/H \\n\\nMore\\n\\nThe Monetary Policy Committee (MPC) unanimously decided to hold the repo rate at 6.5 percent, but raised the full-year inflation forecast to 5.4 percent from 5.1 percent earlier given the supply disruptions due to adverse weather conditions and assuming a normal monsoon, which was largely on expected lines.\\n\\n\"The MPC also decided to remain focused on withdrawal of accommodation to ensure that inflation progressively aligns with the target, while supporting growth,\" the Reserve Bank of India said in its release.\\n\\nStory continues below Advertisement\\n\\nRemove Ad\\n\\nThe MPC has also maintained its real GDP growth forecast for current financial year 2023-24 at 6.5 percent considering the hope for better household consumption, and renewal of the capex cycle going ahead, amid global headwinds.\\n\\n\"There were no surprises in the policy and unlike market expectations, there was no hawkish bend to the commentary,\" Indranil Pan, Chief Economist at YES Bank said.\\n\\nHowever, he further said as any good central bank would do, the RBI signals that it would continue to look ahead and factor in its anticipated inflation trajectory into future monetary policy decisions, and opined that excessive liquidity can pose risks to price stability.\\n\\nRelated stories\\n\\nBroader indices hit record highs; more than 140 smallcaps gain between 10-42%\\n\\nRecord run continued on Dalal Steet as indices gain for fifth straight week\\n\\nTechnical View | Nifty ends flat for second straight session; immediate support at 24,170\\n\\nMeanwhile, the RBI has attempted to neutralize the liquidity impact of the Rs 2000/- notes into the banking system, and has hiked incremental CRR (cash reserve ratio) to 10 percent till September 2023 to address surplus liquidity generated by various factors.\\n\\nThe market has reacted negatively to the rising inflation forecast and incremental CRR for banks announced by the RBI. Hence the Bank Nifty fell 240 points to 44,640, while the BSE Sensex fell 246 points to 65,750, and the Nifty50 dropped 69 points to 19,564, at 12:49 hours IST.\\n\\nBut experts are not worried due to this fall, in fact, it gives an opportunity to pick quality stocks including rate sensitive stocks on dips. They still feel the market is in a consolidation phase and can give decisive breakout over crucial 19,650-19,700 levels soon, as long as it holds the 19,500-19,300 support. Overall it traded within the previous day\\'s range.\\n\\nStory continues below Advertisement\\n\\nRemove Ad\\n\\n\"We believe that a dip in the market is a much needed. The RBI move may pave the way for healthy correction. If Bank Nifty fails to hold above the 44,500 mark then there is a chance of a fall till 43,500. This is a major support level for Bank Nifty where investors can get it to good quality banking stocks,\" Apurva Sheth, head of market perspectives & research at SAMCO Securities said.\\n\\nOn the Nifty50, Osho Krishan, senior. analyst, technical & derivative research at Angel One feels the bearish gap of 19,678-19,705 withholds the immediate hurdle and a decisive breach would attract new longs in the index for the next potential resistance around 19,800-19,850 in the comparable period. While on the downside, the pivotal support of 19,500 proved its mettle and is expected to cushion any blip in the shortcoming, he said.\\n\\nFollow LIVE Updates of the RBI Monetary Policy decision here\\n\\nWe have collated a list of rate-sensitive stocks that may be a good buy at current levels or on dips from a two to three weeks’ perspective. Returns are calculated based on August 9 closing price:\\n\\nExpert: Foram Chheda, CMT, technical research analyst and founder of ChartAnalytics.co.in\\n\\nHDFC Bank: Buy | LTP: Rs 1,647 | Stop-Loss: Rs 1,612 | Target: Rs 1,720 | Return: 4.4 percent\\n\\nAfter marking a high on a closing basis in July this year, HDFC Bank\\'s stock price saw a corrective decline and halted at Rs 1,579-1,580 levels which was in close proximity to the 200-day MA (moving average) marking it as a strong support level.\\n\\nThe price movement after that led to the development of the symmetrical triangle. Currently, the stock is very close to multiple moving average support level at 50-day, 100-day and 200-day MA which is expected to act as a strong support level and the price is likely to move towards the previous highs of Rs 1,720 levels.\\n\\nThus, one should continue to hold the stock with stop-loss of Rs 1,612 and can expect a potential up move towards Rs 1,720 levels.\\n\\nManappuram Finance: Buy | LTP: Rs 142.40 | Stop-Loss: Rs 131 | Target: Rs 155 | Return: 9 percent\\n\\nSince February 2022, Manappuram Finance has been in a consolidation phase. After reaching a low point at Rs 81 levels in June 2022, the stock gradually climbed. During its ascent, it surpassed the 50-day, 100-day, and 200-day moving averages. However, its upward momentum slowed around Rs 133-134 levels, acting as a resistance, and a corrective decline followed. This decline found support near the 200-day moving average. A rebound in the price followed this price action.\\n\\nIn the past month, the price tested the resistance at Rs 131-133 levels multiple times. Finally, a breakout occurred which took the price above the horizontal trendline resistance of Rs 133, creating fresh buying opportunities.\\n\\nPresently, one can hold the stock, maintain a stop-loss near Rs 131, while expecting an upward move towards Rs 155.\\n\\nL&T Finance Holdings: Buy | LTP: Rs 126.2 | Stop-Loss: Rs 119 | Target: Rs 138-140 | Return: 11 percent\\n\\nAfter forming a bottom at Rs 78 levels in March this year, the stock price has witnessed a rebound. Despite a temporary violation, the price defended the 200-day MA as its support by staging a rebound and crossing above it. This reinforced the levels of 200-DMA as a major support for the stock.\\n\\nThe price gradually crossed above multiple moving averages of 50-day MA as well as 100-day MA which concluded the trend of the stock to be bullish. This up move in price halted near Rs 140 levels last month and retraced nearly 23.6 percent as per the Fibonacci retracement levels getting very close to the 50-day MA in the process.\\n\\nThus, the formation of higher tops and higher bottoms, Fibonacci retracement of 23.6 percent, and 50-day MA support level can trigger a rebound in price which could result in a potential continuation of the uptrend. One can continue holding the stock with a stop-loss of Rs 119 and look for a potential up move close to Rs 138-140 levels.\\n\\nExpert: Vidnyan Sawant, AVP - technical research at GEPL Capital\\n\\nICICI Bank: Buy | LTP: Rs 972.7 | Stop-Loss: Rs 940 | Target: Rs 1,040 | Return: 7 percent\\n\\nICICI Bank\\'s stock is currently trading around its all-time high levels and has shown a pattern of forming higher tops and higher bottoms on the monthly charts. This pattern suggests a strong positive sentiment for the stock over the long term. Recently, the stock has found support around the previous swing high, which has now become a new support level due to the concept of change in polarity (CIP). This indicates a bullish signal for the stock, with this support level forming around Rs 960.\\n\\nFurthermore, the stock has been consistently trading above its important moving averages, including the 50-day, 100-day, and 200-day moving averages. This alignment of moving averages in a positive direction indicates the bullish strength of the stock\\'s trend.\\n\\nThe relative strength index (RSI), a momentum indicator, has been indicating positive momentum on various time frames such as daily, weekly, and monthly. It has consistently remained above the 50 mark, reinforcing the presence of a favorable upward momentum in the stock\\'s price.\\n\\nLooking ahead, there is potential for the upward movement of the stock to continue, possibly targeting Rs 1,040 levels. It\\'s recommended to set a stop-loss at Rs 940, which should be triggered on a closing basis.\\n\\nCanara Bank: Buy | LTP: Rs 336.10 | Stop-Loss: Rs 320 | Target: Rs 360 | Return: 7 percent\\n\\nCanara Bank stock has rebounded positively from the critical support level of Rs 325, which shows a change in polarity at the same level. A notable Cup & Handle pattern breakout in early July 2023 suggested the onset of a bullish trend.\\n\\nThis upward trend is further reinforced as the stock prices consistently take support from the 12 & 26-day exponential moving averages (EMA). Additionally, the RSI displays a positive hidden divergence, indicating strong positive momentum in the stock.\\n\\nGoing ahead, we can expect the up move to continue till Rs 360 levels, where the stop-loss must be Rs 320 on closing basis.\\n\\nPNB Housing Finance: Buy | LTP: Rs 634.55 | Stop-Loss: Rs 600 | Target: Rs 700 | Return: 10 percent\\n\\nPNB Housing stock has made an impressive rise from its lower levels around Rs 380, evident from the rising steepness of the trend lines. The stock consistently forms higher highs and higher lows, indicating a sustained upward momentum.\\n\\nThis bullish sentiment is further validated as, during every decline, the stock rebounds from its variable support levels defined by the 12-day and 26-day EMAs.\\n\\nMoreover, the RSI exhibits a Positive Hidden Divergence, underscoring the prevailing positive momentum. Going ahead we can expect the up move to continue till Rs 700 level where the stop-loss must be Rs 600 on closing basis.\\n\\nRBL Bank: Buy | LTP: Rs 220.95 | Stop-Loss: Rs 205 | Target: Rs 270 | Return: 22 percent\\n\\nRBL Bank is currently trading close to its 52-week high, maintaining a higher top higher bottom pattern on the monthly charts, suggesting a positive long-term trend. Additionally, on the weekly chart, the stock has broken out of an Inverted Head & Shoulder pattern and is sustaining above that level, confirming a robust positive sentiment in the medium term. In the short term, the stock seems to be finding support around Rs 205 – 200 levels.\\n\\nMoreover, the stock has consistently traded above key moving averages, including the 50-day, 100-day, and 200-day moving averages. This alignment of moving averages in a positive direction further underscores the bullish trajectory of the stock.\\n\\nThe RSI, a momentum indicator, has consistently indicated positive momentum across various time frames such as daily, weekly, and monthly. The RSI has consistently maintained levels above 55, highlighting the presence of strong upward momentum in the stock\\'s price.\\n\\nLooking forward, there is potential for the stock\\'s upward movement to persist, with a potential target around Rs 270 levels. To manage risk, it is advisable to implement a stop-loss strategy at Rs 205, which should be activated based on closing prices.\\n\\nExpert: Mitesh Karwa, research analyst at Bonanza Portfolio\\n\\nAshok Leyland: Buy | LTP: Rs 187.55 | Stop-Loss: Rs 178 | Target: Rs 207 | Return: 10 percent\\n\\nA significant breakout of a Cup and Handle pattern in Ashok Leyland has been observed. This pattern often indicates a potential bullish continuation of the existing trend. The good volume during the breakout suggests increased buying interest at the current price levels, which further reinforces the positive outlook.\\n\\nMoreover, the price is trading above the major EMA, indicating a sustained uptrend. This alignment with the EMAs adds more confidence to the bullish scenario. Simultaneously, the technical indicator Ichimoku Cloud suggests that the price is trading above the conversion and base line which shows a Positive trend in the counter.\\n\\nHence, based on the above technical structure one can initiate a long position in Ashok Leyland at Rs 187.55 or buy on a dip till Rs 187 levels can be used as buying opportunity for the upside target of Rs 207. However, the bullish view will be negated if Ashok Leyland closes below the support level of Rs 178.\\n\\nHero MotoCorp: Buy | LTP: Rs 3,059.7 | Stop-Loss: Rs 2,850 | Target: Rs 3,500 | Return: 14 percent\\n\\nIn the weekly timeframe, Hero price retraced 0.618 percent from its high after an upward move. The stock found support at the EMA and rebounded, approaching the previous high. This is accompanied by a pattern of higher highs and higher lows, indicating a bullish trend.\\n\\nAdditionally, the price remains above the 100-period EMA and the Ichimoku Indicator, supporting the uptrend. The RSI momentum indicator is also trending, with new highs, confirming the trend.\\n\\nIn light of these technical factors, a long position could be considered at Rs 3,059 or a potential decline to Rs 3,050, with a target price of Rs 3,500. However, this bullish view would be invalidated if the stock closes below the support level of Rs 2,850.\\n\\nBank of Maharashtra: Buy | LTP: Rs 37.3 | Stop-Loss: Rs 30 | Target: Rs 50 | Return: 34 percent\\n\\nBank of Maharashtra has seen breaking out of an Inverse Head and Shoulder pattern on the weekly timeframe with a bullish candlestick pattern and closing above the highs of last six years which indicates strength. The stock is also trading above all its important EMAs on the daily timeframe which acts as a confluence.\\n\\nThe supertrend indicator is also indicating a bullish continuation which supports the bullish view. Momentum oscillator RSI (14) is at around 75 on the daily time frame indicating strength by sustaining above 50 and the ichimoku cloud is also suggesting a bullish move as the price is trading above the conversion line, base line and cloud.\\n\\nObservation of the above factors indicates that a bullish move in Bank of Maharashtra is possible for target upto Rs 50. One can initiate a buy trade in between the range of Rs 37-37.3, with stop-loss of Rs 30 on daily closing basis.\\n\\nDisclaimer: The views and investment tips expressed by investment experts on Moneycontrol.com are their own and not those of the website or its management. Moneycontrol.com advises users to check with certified experts before taking any investment decisions.\\n\\nSunil Shankar Matkar\\n\\nTags:\\n\\n#Market Edge\\n\\n#Nifty\\n\\n#RBI monetary policy\\n\\n#Sensex\\n\\n#Stocks Views\\n\\nfirst published: Aug 10, 2023 01:38 pm\\n\\nTop Trends\\n\\nBudget 2024\\n\\nWipro\\n\\nStock Radar\\n\\nIxigo IPO allotment\\n\\nIPO News\\n\\nPrajwal Revanna\\n\\nAdvertisement\\n\\nRemove Ad\\n\\nTrending news\\n\\nCaptain Anshuman Singh\\'s wife collects Kirti Chakra, recalls their story: \\'It was love at first sight\\'\\n\\n38-year-old TikTok executive, marathon-runner diagnosed with stage 4 cancer, reveals symptoms\\n\\nAnanya Birla\\'s reply to man calling her \\'over-enthusiastic woman\\' gets a thumbs up. Viral\\n\\nBillionaire Harsh Mariwala visits supermarkets regularly. Here\\'s why\\n\\nWoman poses as live mannequin at Dubai mall, sparks debate: \\'This is inhuman\\'\\n\\nAdvisory Alert:\\n\\ngrievanceofficer@nw18.com or call on 02268882347\\n\\nForum\\n\\nFacebook\\n\\nTwitter\\n\\nInstagram\\n\\nLinkedin\\n\\nRSS\\n\\nPortfolio\\n\\nMarkets\\n\\nWatchlist\\n\\nLive TV Show\\n\\nCurrencies\\n\\nFREE Credit Score₹100 Cash Reward\\n\\nCommodities\\n\\nFixed Income\\n\\nPersonal Finance\\n\\nMutual Fund\\n\\nPre-Market\\n\\nIPO\\n\\nGlobal Market\\n\\nBudget 2024\\n\\nElections 2024\\n\\nGold Rate\\n\\nBSE Sensex\\n\\nForum\\n\\nMC 30\\n\\nNews\\n\\nBusiness\\n\\nMarkets\\n\\nStocks\\n\\nIncome Tax Calculator\\n\\nIndia News\\n\\nT20 World Cup Points Table\\n\\nITR Filing 2023-24\\n\\nEconomy\\n\\nMutual Funds\\n\\nPersonal Finance\\n\\nIPO News\\n\\nStartups\\n\\nStocks:\\n\\nA |\\n\\nB |\\n\\nC |\\n\\nD |\\n\\nE |\\n\\nF |\\n\\nG |\\n\\nH |\\n\\nI |\\n\\nJ |\\n\\nK |\\n\\nL |\\n\\nM |\\n\\nN |\\n\\nO |\\n\\nP |\\n\\nQ |\\n\\nR |\\n\\nS |\\n\\nT |\\n\\nU |\\n\\nV |\\n\\nW |\\n\\nX |\\n\\nY |\\n\\nZ |\\n\\nOthers\\n\\nMutual Funds:\\n\\nA \\r\\n             |\\n\\nB \\r\\n             |\\n\\nC \\r\\n             |\\n\\nD \\r\\n             |\\n\\nE \\r\\n             |\\n\\nF \\r\\n             |\\n\\nG \\r\\n             |\\n\\nH \\r\\n             |\\n\\nI \\r\\n             |\\n\\nJ \\r\\n             |\\n\\nK \\r\\n             |\\n\\nL \\r\\n             |\\n\\nM \\r\\n             |\\n\\nN \\r\\n             |\\n\\nO \\r\\n             |\\n\\nP \\r\\n             |\\n\\nQ \\r\\n             |\\n\\nR \\r\\n             |\\n\\nS \\r\\n             |\\n\\nT \\r\\n             |\\n\\nU \\r\\n             |\\n\\nV \\r\\n             |\\n\\nW \\r\\n             |\\n\\nX \\r\\n             |\\n\\nY \\r\\n             |\\n\\nVisit the App Store to see all our apps:\\n\\nDownload from Google Play\\n\\nDownload from APP Store\\n\\nDownload from Windows Phone\\n\\nTools\\n\\nRetirement Planning\\n\\nEMI Calculator\\n\\nSIP Calculator\\n\\nSIP Planner\\n\\nUseful Links\\n\\nCrypto News\\n\\nBank Holidays in India\\n\\nGold Rate Today\\n\\nSilver Rate Today\\n\\nTrending News\\n\\nStartups\\n\\nNational News\\n\\nMC Videos\\n\\nMC You Tube\\n\\nHouse Purchase Index\\n\\nBest Portfolio Manager\\n\\nSmall Savings Schemes\\n\\nBonds\\n\\nTopperLearning\\n\\nClear Study Doubts\\n\\nEducation Franchisee Opportunity\\n\\nSpecials\\n\\nMaster Your Money\\n\\nGame Changers\\n\\nInvestment Watch\\n\\nPowerYourTrade\\n\\nMoneyBhai\\n\\nFocus\\n\\nSME Step Up\\n\\nNetwork 18 Sites\\n\\nNews18\\n\\nFirstpost\\n\\nCNBC TV18\\n\\nNews18 Hindi\\n\\nCricketnext\\n\\nOverdrive\\n\\nTopper Learning\\n\\nAbout us |\\n\\nContact Us |\\n\\nAdvisory Alert |\\n\\nAdvertise with Us |\\n\\nSupport |\\n\\nDisclaimer |\\n\\nPrivacy Policy |\\n\\nCookie Policy |\\n\\nTerms & Conditions |\\n\\nCareers |\\n\\nFinancial Terms (Glossary) |\\n\\nFAQs |\\n\\nSitemap |\\n\\nRSS Feed |\\n\\nInvestors\\n\\nCopyright © e-Eighteen.com Ltd. All rights reserved. Reproduction of news articles, photos, videos or any other content in whole or in part in any form \\r\\n        or medium without express writtern permission of moneycontrol.com is prohibited.\\n\\nYou got 30 Day’s Trial of\\n\\nGrab it Now\\n\\nAd-Free Experience\\n\\nTop Articles from Financial Times\\n\\nActionable Insights\\n\\nMC Research\\n\\nEconomic Calendar\\n\\n& Many More\\n\\nYou are already a Moneycontrol Pro user.\\n\\nSunil Matkar'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "loader = UnstructuredURLLoader(\n",
    "    urls = [\n",
    "        \"https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html\",\n",
    "        \"https://www.moneycontrol.com/news/business/markets/market-corrects-post-rbi-ups-inflation-forecast-icrr-bet-on-these-top-10-rate-sensitive-stocks-ideas-11142611.html\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = loader.load()\n",
    "text = data[1].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660\n",
      "928\n",
      "995\n",
      "997\n",
      "257\n",
      "643\n",
      "752\n",
      "769\n",
      "993\n",
      "935\n",
      "942\n",
      "804\n",
      "804\n",
      "883\n",
      "894\n",
      "919\n",
      "733\n",
      "995\n",
      "847\n",
      "808\n",
      "713\n",
      "781\n",
      "996\n",
      "741\n",
      "919\n",
      "992\n",
      "985\n",
      "983\n",
      "987\n",
      "901\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\",\"\\n\",\" \"],  # List of separators based on requirement (defaults to [\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunk_size = 1000,  \n",
    "    chunk_overlap  = 100, \n",
    "    length_function = len  # Function to calculate size, currently we are using \"len\" which denotes length of string however you can pass any token counter)\n",
    ")\n",
    "\n",
    "chunks = r_splitter.split_text(text)\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 768)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "vectors = encoder.encode(chunks)\n",
    "\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import ServerlessSpec\n",
    "\n",
    "# cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "# region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "# spec = ServerlessSpec(cloud=cloud, region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index_name = 'tutor'\n",
    "# pc.describe_index(index_name).status['ready']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index = pc.Index(index_name)\n",
    "# index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DirectoryLoader('artifacts', )\n",
    "loader = PyMuPDFLoader(r'C:/vidit/codes/PROJECTS/Tutor-app/backend/knowledge_store/transformer_research_paper.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [docs[i].page_content for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n',\n",
       " '1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\n',\n",
       " 'Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\n',\n",
       " 'Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\n',\n",
       " 'output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\n',\n",
       " 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\n',\n",
       " 'length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\n',\n",
       " 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\n',\n",
       " 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\n',\n",
       " 'Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n',\n",
       " '[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n',\n",
       " '[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\n',\n",
       " 'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\n',\n",
       " 'Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\n',\n",
       " 'Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"tutor\"\n",
    "\n",
    "# Split our documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\",\"\\n\",\" \"], \n",
    "    chunk_size = 200,  \n",
    "    chunk_overlap  = 20, \n",
    "    length_function = len \n",
    ")\n",
    "split_docs = text_splitter.split_text(docs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "198\n",
      "176\n",
      "130\n",
      "153\n",
      "155\n",
      "158\n",
      "153\n",
      "161\n",
      "164\n",
      "114\n",
      "107\n",
      "111\n",
      "112\n",
      "109\n",
      "109\n",
      "111\n",
      "113\n",
      "165\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "for chunk in split_docs:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "import dotenv \n",
    "\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunks_in_pinecone(chunks,index,namespace):\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        text = chunk.page_content\n",
    "        text = text.replace(\"\\n\",\". \")\n",
    "        text = text.replace(\"\\t\",\". \")\n",
    "\n",
    "        vector = encoder.encode(text)\n",
    "        vector_id = f\"doc_chunk_{idx}\"\n",
    "        \n",
    "        index.upsert(\n",
    "            vectors = \n",
    "            [\n",
    "                {\n",
    "                    \"id\" : vector_id,\n",
    "                    \"values\" : vector,\n",
    "                    \"metadata\" : {'source': '','text' : text}\n",
    "                }\n",
    "            ] ,\n",
    "            namespace=namespace,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "VectorDictionaryMissingKeysError",
     "evalue": "Vector dictionary is missing required fields: ['values']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mVectorDictionaryMissingKeysError\u001b[0m          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstore_chunks_in_pinecone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[213], line 10\u001b[0m, in \u001b[0;36mstore_chunks_in_pinecone\u001b[1;34m(chunks, index, namespace)\u001b[0m\n\u001b[0;32m      7\u001b[0m vector \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[0;32m      8\u001b[0m vector_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvaluess\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m,\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\utils\\error_handling.py:11\u001b[0m, in \u001b[0;36minner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ProtocolError):\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to connect to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; did you specify the correct index name?\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\index.py:170\u001b[0m, in \u001b[0;36mupsert\u001b[1;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upsert_batch(vectors, namespace, _check_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size must be a positive integer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vectors), disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpserted vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\index.py:197\u001b[0m, in \u001b[0;36m_upsert_batch\u001b[1;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespace)])\n\u001b[0;32m    187\u001b[0m vec_builder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_api\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    190\u001b[0m     UpsertRequest(\n\u001b[0;32m    191\u001b[0m         vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(vec_builder, vectors)),\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict,\n\u001b[0;32m    193\u001b[0m         _check_type\u001b[38;5;241m=\u001b[39m_check_type,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    195\u001b[0m     ),\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[1;32m--> 197\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\index.py:193\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    186\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_non_empty_args([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamespace\u001b[39m\u001b[38;5;124m\"\u001b[39m, namespace)])\n\u001b[0;32m    187\u001b[0m vec_builder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m v: VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_api\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    190\u001b[0m     UpsertRequest(\n\u001b[0;32m    191\u001b[0m         vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(vec_builder, vectors)),\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict,\n\u001b[1;32m--> 193\u001b[0m         _check_type\u001b[38;5;241m=\u001b[39m_check_type,\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    195\u001b[0m     ),\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS},\n\u001b[0;32m    197\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\vector_factory.py:28\u001b[0m, in \u001b[0;36mbuild\u001b[1;34m(item, check_type)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39m_tuple_to_vector(item, check_type)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Mapping):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39m_dict_to_vector(item, check_type)\n",
      "File \u001b[1;32mc:\\Users\\vidit\\.conda\\envs\\tutor\\lib\\site-packages\\pinecone\\data\\vector_factory.py:48\u001b[0m, in \u001b[0;36mVectorFactory._dict_to_vector\u001b[1;34m(item, check_type)\u001b[0m\n\u001b[0;32m     46\u001b[0m item_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(item\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m item_keys\u001b[38;5;241m.\u001b[39missuperset(REQUIRED_VECTOR_FIELDS):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m VectorDictionaryMissingKeysError(item)\n\u001b[0;32m     50\u001b[0m excessive_keys \u001b[38;5;241m=\u001b[39m item_keys \u001b[38;5;241m-\u001b[39m (REQUIRED_VECTOR_FIELDS \u001b[38;5;241m|\u001b[39m OPTIONAL_VECTOR_FIELDS)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(excessive_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mVectorDictionaryMissingKeysError\u001b[0m: Vector dictionary is missing required fields: ['values']"
     ]
    }
   ],
   "source": [
    "store_chunks_in_pinecone(docs, index,\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = [0.0828915909, -0.0109610604, -0.0137331225, -0.0420022, -0.032611683, -0.0134577537, 0.103452846, 0.0377742872, -0.00628403854, -0.0305299442, -0.0061371671, 0.0326002352, 0.0459518433, 0.0114742322, -0.00552665861, 0.0124429129, 0.0373903438, 0.0424919911, 0.0218762346, 0.00220496743, -0.0544621833, -0.017204307, 0.0353284217, 0.019594159, 0.0374116637, -0.011419653, 0.0510954261, -0.041959811, 0.000983059872, -0.0609145872, 0.0540829152, 0.0160237011, -0.0157552157, 0.0289409012, 0.00000249528011, -0.0324194394, 0.00245248945, 0.0450812727, 0.0368107073, 0.0779069215, 0.0405737422, 0.00738166273, -0.0173286088, -0.01309558, -0.0236868821, -0.0568953864, 0.00794750825, 0.0414108261, -0.0479017571, 0.00768168364, 0.0133750243, 0.00454847841, 0.0618613809, 0.00428978261, 0.0251422245, -0.0187372174, 0.0271532815, 0.039297428, 0.0212325864, 0.0273760725, -0.0178202782, 0.0224938076, -0.0267114155, -0.0221465472, 0.0119918482, 0.0473494716, 0.0185042396, -0.0085211955, 0.0634914, -0.0066223382, 0.130368277, -0.0322167091, 0.0297477879, 0.0405103, -0.0434983522, 0.0461343862, 0.00528288214, 0.0106965285, -0.0366933458, -0.0281769726, 0.0669987574, 0.0634374321, -0.00741368346, 0.0196955092, 0.00154642027, 0.0042987559, -0.0307235, -0.0246864948, 0.0174652375, -0.0502572693, 0.0581222735, -0.0294795334, -0.0107105272, -0.0108148977, 0.0246086381, -0.0348517373, 0.0761027, -0.0309510883, 0.0112329079, -0.0338761918, -0.0438834205, 0.00718821632, 0.028786201, 0.054776378, 0.0407296233, -0.0304144565, -0.0253766868, 0.00251991116, 0.0257006083, 0.0196139105, -0.0337911695, -0.0172141045, 0.0599756725, -0.00442103762, 0.0282742921, -0.0115725799, -0.0976236, -0.0102020465, 0.00874845497, -0.00846910942, -0.00182691205, 0.031970121, -0.0476845205, 0.0217210986, -0.0266993623, 0.021790674, -0.0273364019, 0.0137447491, -0.0218967702, 0.0357382298, 0.0292600282, 0.0125802131, 0.0219685212, -0.0446110331, 0.0116280103, -0.00471792324, -0.0124533288, -0.0141091198, -0.0197460894, 0.0097604366, -0.00561201898, -0.0441992171, 0.0142085021, -0.00680718664, 0.00894029066, 0.01683194, 0.00221669418, -0.0383608378, -0.0198190119, -0.0178433545, -0.0218266491, -0.0308956932, -0.0612981319, -0.00323503488, 0.0534549467, 0.0253105629, 0.0374983102, -0.0391343422, 0.00552831264, -0.0133157214, 0.00770097272, -0.0047106077, 0.030048484, -0.0472838283, -0.0348886959, 0.0164161809, 0.0179464053, 0.014905489, -0.0374043956, -0.0382702313, -0.0117096566, 0.0000637193734, -0.0407352671, 0.065715313, 0.0600267351, -0.00514716, -0.00669236062, 0.0250033084, -0.0202844758, 0.00544328289, 0.0489200577, -0.0530171394, 0.0426089242, 0.0248634741, 0.030221073, 0.00573079893, 0.019111922, -0.00370656815, 0.0119371647, -0.038386941, 0.0111349029, 0.0195080657, -0.0211844575, -0.000547668, 0.041678533, -0.0206953846, 0.0383029, -0.0298785809, -0.00102731318, 0.00113354716, -0.0429773144, 0.0219402853, 0.0485343598, 0.0192676447, 0.00218791049, -0.0154087776, 0.054482352, 0.00936010201, -0.0227399543, 0.000769048929, -0.0650154352, 0.0298100542, 0.0122903334, 0.00514974725, 0.00206985231, 0.0268148668, -0.00893883314, 0.03822143, 0.00157296564, 0.0922141522, -0.00306454208, -0.00289166137, -0.0139496559, -0.0266718846, -0.0318369865, 0.018904414, -0.00300925458, -0.0219890233, 0.0198752768, 0.0342664234, -0.0649806559, -0.0103302663, 0.0309895203, -0.02069778, -0.0298205167, 0.0246445257, 0.0882784575, 0.0927725211, -0.00910731126, -0.0783329681, 0.0386198, -0.0270952154, -0.00508834654, -0.0128751807, 0.0177393053, 0.0222371276, 0.031462796, -0.00596398069, -0.00233020214, -0.038216114, 0.00742547726, 0.0133467689, 0.0558220185, 0.0000948565721, 0.0407240167, 0.0451214574, -0.0706777573, 0.0441380367, -0.0103473477, 0.0200132374, 0.0272670425, -0.0350551829, -0.0249535386, 0.060829334, 0.0208323, -0.0104087433, -0.0337109081, 0.0048388876, -0.0253377501, -0.00130462612, 0.00602822611, 0.000492582447, -0.0214995053, -0.00686765, 0.0423609242, 0.022223508, -0.0213092826, -0.0101239597, -0.0240998846, 0.0583113022, 0.00708251074, 0.039138943, -0.0180068482, 0.000741007738, -0.0151908509, 0.00834727474, 0.0928725898, -0.0786776394, -0.0146090705, -0.0100790346, -0.0197962075, 0.00214834255, -0.0528732, -0.0267982837, -0.032395862, 0.0205077808, -0.0460615754, 0.0331781842, 0.00981911737, 0.03015537, 0.0120492848, -0.0245251693, 0.00464925868, 0.0202805735, -0.00497305393, 0.0479407758, 0.0236330517, -0.048376739, 0.0364764407, -0.0225650724, -0.000130715664, 0.0075394623, 0.0342293642, -0.0228398684, -0.0324904323, -0.0235041492, -0.00646145642, -0.03251715, -0.0203082729, 0.081550993, 0.00153021864, -0.00206605089, -0.0334982648, -0.00809168536, 0.030266894, -0.0391974449, -0.0099335257, -0.0118145011, 0.0240481459, -0.00442961464, 0.0175569393, 0.0457432158, -0.0392796732, -0.0067628338, 0.0264154114, 0.0228378642, -0.0177046936, -0.0636621639, 0.0535767116, -0.0243067518, -0.0520129129, 0.00530843483, 0.031953264, -0.0202128626, 0.0340579711, -0.0334593393, 0.0830619633, -0.0499099605, -0.00812749192, 0.0358885, 0.10766121, -0.0050334828, -0.0213699285, -0.0200720057, -0.00668251142, -0.00839580409, 0.0234798715, -0.0212494228, 0.00975055527, 0.0343365446, -0.0448318683, -0.00179071655, -0.0377485454, 0.0280090868, 0.0397661664, 0.01969992, 0.0642770082, -0.00914323796, -0.081419386, -0.00426997803, 0.0350345634, -0.0289934594, 0.0582522489, 0.0742021278, 0.00769395707, -0.0722953677, -0.0337828733, -0.0300511066, -0.0644483119, 0.00320890103, 0.0134028019, -0.0596864708, 0.092065841, -0.0176240131, -0.0338158831, -0.0538319312, -0.0439913459, -0.0342930034, -0.016865395, 0.0233177748, -0.029398039, -0.0238331966, -0.00499245757, -0.0232033, -0.0461444929, -0.00228560483, -0.0148366624, 0.000474646484, -0.016392041, 0.0102899512, 0.0187038295, -0.0883723199, -0.0166596547, 0.033092171, 0.000605391222, -0.0119482577, 0.0250517931, 0.0497918464, -0.066084072, 0.00411224086, -0.052153334, -0.0230224412, -0.0205028933, -0.0148822917, 0.0664010048, -0.0353399254, 0.00896058325, -0.0370276161, 0.00433198642, 0.00703231944, 0.0446652882, 0.00540511915, 0.0070884116, 0.00687262043, 0.0355492309, -0.0237096027, 0.00892738812, -0.0174259469, -0.0189707968, -0.0185533054, -0.0434698202, 0.0249076057, -0.0061862031, 0.0500153787, -0.0233756695, 0.022243714, 0.00375360949, -0.0341175869, -0.0515319705, -0.0368419513, -0.0715411529, -0.0425350331, 0.0568428524, 0.0294694174, 0.0601997674, 0.012903708, -0.0409387499, 0.0129674328, -0.0973280519, 0.0253901575, 0.00332091, -0.0438595824, 0.0156236086, 0.0347611755, 0.00976946764, -0.0179446023, -0.0526299402, 0.021516541, 0.0019031188, -0.0269328281, 0.0266715102, 0.0264121685, -0.0458507575, 0.00742622698, -0.00239242823, 0.00101631088, 0.0215476118, 0.00383561244, -0.0142397527, -0.0085424874, -0.0620511658, -0.0248422232, -0.0426566117, -0.0112887435, -0.0201841239, 0.000690475514, -0.0559922718, -0.0126789883, -0.0183034912, 0.0273448881, 0.0103349909, 0.0147193838, -0.0460172556, 0.0274905954, 0.0325787477, 0.00632856414, -0.0390700288, -0.00397625053, -0.00264929468, -0.0513475165, 0.0447640233, -0.0105459429, -0.0250227824, 0.107420661, 0.0493374392, 0.0271743797, -0.0581548028, 0.0659172535, -0.0567341708, 0.096310012, -0.0136204939, 0.00892539, 0.000444213278, -0.027534876, 0.00814599, 0.0484181643, -0.00172533677, 0.0396861956, -0.0123282084, -0.0223322678, 0.0117884474, 0.0236529931, -0.0928098112, 0.010599046, 0.0513251051, -0.0394491926, 0.0715423152, -0.000702124147, 0.0252006035, -0.0187341738, 0.0190776754, -0.00374436, 0.0242118519, 0.000415435614, 0.0250112396, -0.0622649, 0.00252862368, -0.0274931267, 0.0391479656, 0.0743277818, 0.0972099, -0.0616396628, 0.00200561038, -0.0558071248, 0.0217129104, -0.0134909321, 0.0102222189, 0.0610636771, 0.00289361388, 0.0140922582, -0.0253831036, -0.0203524046, 0.0109083308, -0.00374992541, -0.000384735147, -0.0503201596, -0.0329827666, 0.0136934193, 0.0464733131, -0.0360739492, -0.0295595676, -0.00903569628, -0.0639803484, 0.00282565528, 0.0342626, -7.62020752e-33, 0.0348942503, -0.0359011553, 0.0247418545, 0.00481113093, -0.0480940789, 0.0204219855, -0.0298511237, -0.0145599395, -0.0668907687, 0.00219926517, -0.0805463418, -0.00805195235, 0.0092622079, -0.00357259717, 0.0266291741, 0.0633839443, -0.0376900695, 0.0262610838, -0.00238141813, -0.0342469, 0.024246892, 0.0110699488, 0.0624549277, -0.0328984931, 0.0583810098, 0.00357276504, -0.016244024, -0.0458295718, 0.0472441129, 0.00207011984, 0.0191175025, -0.0320127457, 0.0225738958, -0.0552259646, 0.0166958049, -0.0610796548, -0.0919538215, -0.00636024307, -0.00634803157, 0.0279138889, 0.0216747113, 0.0034947542, 0.0425596908, -0.00329550682, -0.0315621234, 0.00323228654, 0.0611003824, 0.00199710461, -0.0870301127, -0.0540871099, 0.000970375841, -0.0131091615, 0.00164378132, 0.121977106, -0.0389591493, 0.0458402187, -0.0251122471, 0.0212387424, -0.0649295747, 0.00582447182, 0.00707374467, 0.0893055126, 0.0159334373, -0.0100898268, 0.0304512791, -0.00093530817, 0.023985086, 0.0190823935, 0.0197523236, -0.0207654797, -0.00747683551, 0.0139080603, -0.066028, 0.0985070094, 0.0137844067, -0.0940804183, -0.0491746664, 0.0425573625, 0.0581045821, -0.00544359349, 0.0117936283, -0.0209728722, -0.0314563923, -0.0466105305, 0.0473259389, 0.054147, 0.0107649788, -0.0359382778, 0.026368605, -0.0455537923, 0.0161353145, 0.0107065896, 0.0123144258, -0.0581069179, -0.0335470065, 0.0623139814, -0.0124190068, 0.0442875326, -0.0259920452, -0.0131240385, -0.0927172899, -0.0496345349, -0.0505607463, 0.0635729581, 0.00326133496, -0.00466958713, 0.0164645128, 0.0162077341, -0.038486477, -0.0229508467, 0.034288466, -0.0204868261, 0.0391127951, -0.0133459624, -0.0589093193, -0.00451791566, 0.0327330343, -0.0386987664, 0.0757958144, 0.0381116681, -0.00948850438, -0.0517759435, -0.044844836, -0.0000633628, 0.00875712372, 0.00309004728, -0.0462804697, 0.0450426228, 0.00800026581, -0.0119164707, -0.00346010807, -0.049270045, 3.29097475e-7, 0.0234622192, 0.0054900567, -0.0140785202, -0.0101590017, 0.0277022347, -0.0164527614, -0.0120923398, 0.0406606905, 0.0118274484, 0.0376342498, 0.0650030822, -0.0213564206, -0.0200665742, -0.00323811709, -0.0563624725, -0.0227815937, -0.0933469906, -0.0223109554, -0.0173683576, 0.0274538212, -0.0341707915, -0.0232605971, -0.0177894924, 0.00316540082, -0.00475196401, -0.0581303351, 0.00237789494, -0.0555701554, 0.0113115786, 0.0366914943, 0.0145752663, -0.0110980654, -0.00587131316, 0.0175981093, -0.0154663716, -0.0141303912, -0.00546650076, -0.000582889712, 0.0167552233, 0.0747121796, 0.0100347968, -0.0366787426, 0.0109702768, -0.0450229645, 0.0660410598, 0.0528041087, -0.0850212499, 0.0244038124, -0.0467818193, -0.0398042537, 0.012778651, -0.00283739157, 0.0135533772, 0.00978401, -0.0348206721, 0.0219416525, 0.00742749, 0.0121356435, -0.00928652193, -0.00523139816, 0.00130310119, -0.0798810869, -0.00906001683, -0.0293319337, 0.0363927968, 0.012738592, 0.0156276394, 3.37076472e-34, 0.0126905199, -0.0333545916, -0.0191886947, 0.00520856306, 0.0222699642, -0.0139833856, -0.055403892, -0.00884083379, -0.0148334522, -0.0063038012, -0.0276833083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'doc_chunk_4', 'score': 1.0, 'values': []},\n",
       "             {'id': 'doc_chunk_6', 'score': 0.843322873, 'values': []},\n",
       "             {'id': 'doc_chunk_2', 'score': 0.743641257, 'values': []},\n",
       "             {'id': 'doc_chunk_5', 'score': 0.556426167, 'values': []},\n",
       "             {'id': 'doc_chunk_8', 'score': 0.522498786, 'values': []},\n",
       "             {'id': 'doc_chunk_1', 'score': 0.509563506, 'values': []},\n",
       "             {'id': 'doc_chunk_0', 'score': 0.46634388, 'values': []},\n",
       "             {'id': 'doc_chunk_3', 'score': 0.444756329, 'values': []},\n",
       "             {'id': 'doc_chunk_10', 'score': 0.417294651, 'values': []},\n",
       "             {'id': 'doc_chunk_7', 'score': 0.387660056, 'values': []}],\n",
       " 'namespace': 'test',\n",
       " 'usage': {'read_units': 5}}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_results1 = index.query(\n",
    "    namespace=\"test\",\n",
    "    vector=test_vector,\n",
    "    top_k=10,\n",
    "    # include_values=True\n",
    "    # include_metadata=True\n",
    ")\n",
    "\n",
    "query_results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index.delete(delete_all=True, namespace=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\vidit\\\\codes\\\\PROJECTS\\\\Tutor-app'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 9bdb9191-1581-48eb-ba0e-42b493304fe8\n",
      "Started parsing the file under job_id ef27e893-e5e0-4909-9095-913c766809a2\n",
      "Loaded 2 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "from llama_parse import LlamaParse\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "llama_api_key = os.getenv(\"LLAMA_API_KEY\")\n",
    "parser = LlamaParse(api_key=llama_api_key, result_type=\"markdown\")    # Better than pypdf\n",
    "\n",
    "file_extractor = {\".pdf\": parser,\".csv\":parser,\".docx\":parser,\".txt\":parser}\n",
    "documents = SimpleDirectoryReader(\"./artifacts/csvs\",file_extractor=file_extractor).load_data()  \n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaParse' object has no attribute 'parse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./artifacts/pdf/SSRN-id4878177.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LlamaParse' object has no attribute 'parse'"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from pydantic import BaseModel\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from prompts import context, code_parser_template\n",
    "from code_reader import code_reader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "\n",
    "\n",
    "vector_store = PineconeVectorStore(pinecone_index=pc_index_instance)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    data, \n",
    "    storage_context=storage_context,\n",
    "    embed_model=embedding_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
